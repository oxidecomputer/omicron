// This Source Code Form is subject to the terms of the Mozilla Public
// License, v. 2.0. If a copy of the MPL was not distributed with this
// file, You can obtain one at https://mozilla.org/MPL/2.0/.

//! The entire state of our test system

use crate::nexus::{
    NexusConfig, NexusOp, NexusReply, NexusState, NexusStateDiff,
};
use crate::{Event, member_universe};
use bootstore::schemes::v0::SharePkgCommon;
use daft::{BTreeMapDiff, BTreeSetDiff, Diffable, Leaf};
use iddqd::IdOrdMap;
use omicron_uuid_kinds::GenericUuid;
use secrecy::ExposeSecretMut;
use sled_hardware_types::Baseboard;
use slog::{Logger, info};
use std::collections::{BTreeMap, BTreeSet};
use std::fmt::Display;
use trust_quorum_protocol::{
    BaseboardId, Configuration, CoordinatingMsg, CoordinatorOperation,
    CoordinatorStateDiff, Envelope, Epoch, LoadRackSecretError, Node,
    NodeCallerCtx, NodeCommonCtx, NodeCtx, NodeCtxDiff, NodeDiff, PeerMsgKind,
    PersistentState, ValidatedLrtqUpgradeMsgDiff, ValidatedReconfigureMsgDiff,
};

// The state of our entire system including the system under test and
// test specific infrastructure.
#[derive(Debug, Clone, Diffable)]
pub struct TqState {
    /// A logger for our test
    #[daft(ignore)]
    pub log: Logger,

    /// Our system under test
    pub sut: Sut,

    /// All in flight messages between nodes
    pub bootstrap_network: BTreeMap<BaseboardId, Vec<Envelope>>,

    /// All in flight responses to nexus. We don't model the requests, as those
    /// are `Node` public method calls. But we don't want to synchronously
    /// update nexus state as a result of those calls, because that ruins any
    /// possible interleaving with other actions.
    ///
    /// This is a way to allow interleaving of nexus replies without changing
    /// the Node API to accept a separate set of Nexus messages and return
    /// messages. We may decide that we want to do that, but for now we'll stick
    /// with a concrete `Node` method based API that is "triggered" by nexus
    /// messages.
    pub underlay_network: Vec<NexusReply>,

    /// A model of Nexus's view of the world during the test
    pub nexus: NexusState,

    /// A cache of our member universe, so we only have to generate it once
    pub member_universe: Vec<BaseboardId>,

    /// All possible system faults in our test
    pub crashed_nodes: BTreeSet<BaseboardId>,

    /// All configurations ever generated by a coordinator.
    ///
    /// If an epoch got skipped due to a crashed coordinator then there will not
    /// be a configuration for that epoch.
    pub all_coordinated_configs: IdOrdMap<Configuration>,

    /// Expunged nodes cannot be added to a cluster. We never reuse nodes in
    /// this test. We include nodes here that may not know yet that they have
    /// been expunged in the `Sut`.
    pub expunged: BTreeSet<BaseboardId>,
}

impl TqState {
    pub fn new(log: Logger) -> TqState {
        // We'll fill this in when applying the initial_config
        let sut = Sut::empty();
        let member_universe = vec![];
        TqState {
            log,
            sut,
            bootstrap_network: BTreeMap::new(),
            underlay_network: Vec::new(),
            nexus: NexusState::new(),
            member_universe,
            crashed_nodes: BTreeSet::new(),
            all_coordinated_configs: IdOrdMap::new(),
            expunged: BTreeSet::new(),
        }
    }

    /// Send the latest `ReconfigureMsg` from `Nexus` to the coordinator node
    pub fn send_reconfigure_msg(&mut self) {
        let (coordinator, msg) = self.nexus.reconfigure_msg_for_latest_config();
        let epoch_to_config = msg.epoch;
        if !self.crashed_nodes.contains(coordinator) {
            let (node, ctx) = self
                .sut
                .nodes
                .get_mut(coordinator)
                .expect("coordinator exists");

            node.coordinate_reconfiguration(ctx, msg)
                .expect("valid configuration");

            // Do we have a `Configuration` for this epoch yet?
            //
            // For most reconfigurations, shares for the last committed
            // configuration must be retrieved before the configuration is
            // generated and saved in the persistent state.
            let latest_persisted_config =
                ctx.persistent_state().latest_config().expect("config exists");
            if latest_persisted_config.epoch == epoch_to_config {
                // Save the configuration for later
                self.all_coordinated_configs
                    .insert_unique(latest_persisted_config.clone())
                    .expect("unique");
            }
        }
    }

    /// Send the latest `LrtqUpgradeMsg` from nexus to the coordinator node
    pub fn send_lrtq_upgrade_msg(&mut self) {
        let (coordinator, msg) =
            self.nexus.lrtq_upgrade_msg_for_latest_config();
        if !self.crashed_nodes.contains(coordinator) {
            let (node, ctx) = self
                .sut
                .nodes
                .get_mut(coordinator)
                .expect("coordinator exists");

            node.coordinate_upgrade_from_lrtq(ctx, msg)
                .expect("valid configuration");
        }
    }

    /// Check postcondition assertions after initial configuration
    pub fn postcondition_initial_configuration(&mut self) {
        let (coordinator, msg) = self.nexus.reconfigure_msg_for_latest_config();

        // The coordinator should have received the `ReconfigureMsg` from Nexus
        if !self.crashed_nodes.contains(coordinator) {
            let (node, ctx) = self
                .sut
                .nodes
                .get_mut(coordinator)
                .expect("coordinator exists");
            let mut connected_members = 0;
            // The coordinator should start preparing by sending a `PrepareMsg` to all
            // connected nodes in the membership set.
            for member in msg
                .members
                .iter()
                .filter(|&id| {
                    !self.crashed_nodes.contains(id) && id != ctx.platform_id()
                })
                .cloned()
            {
                connected_members += 1;
                let msg_found = ctx.envelopes().any(|envelope| {
                    envelope.to == member
                        && envelope.from == *coordinator
                        && matches!(
                            envelope.msg.kind,
                            PeerMsgKind::Prepare { .. }
                        )
                });
                assert!(msg_found);
            }
            assert_eq!(connected_members, ctx.envelopes().count());

            // The coordinator should be in the prepare phase
            let cs = node.get_coordinator_state().expect("is coordinating");
            assert!(matches!(cs.op(), CoordinatorOperation::Prepare { .. }));

            // The persistent state should have changed
            assert!(ctx.persistent_state_change_check_and_reset());
            assert!(ctx.persistent_state().has_prepared(msg.epoch));
            assert!(ctx.persistent_state().latest_committed_epoch().is_none());
        }
    }

    /// Put any outgoing coordinator messages from the latest configuration on the wire
    pub fn send_envelopes_from_coordinator(&mut self) {
        let coordinator = {
            let (coordinator, _) =
                self.nexus.reconfigure_msg_for_latest_config();
            coordinator.clone()
        };
        self.send_envelopes_from(&coordinator);
    }

    pub fn send_envelopes_from(&mut self, id: &BaseboardId) {
        let (_, ctx) = self.sut.nodes.get_mut(id).expect("node exists");
        // Only send envelopes to alive nodes
        for envelope in ctx
            .drain_envelopes()
            .filter(|e| !self.crashed_nodes.contains(&e.to))
        {
            let msgs =
                self.bootstrap_network.entry(envelope.to.clone()).or_default();
            msgs.push(envelope);
        }
    }

    pub fn apply_event(&mut self, event: Event) {
        match event {
            Event::InitialSetup {
                member_universe_size,
                config,
                crashed_nodes,
            } => {
                self.apply_event_initial_config(
                    member_universe_size,
                    config,
                    crashed_nodes,
                );
            }
            Event::InitialSetupLrtq { member_universe_size, config } => {
                self.apply_event_initial_config_lrtq(
                    member_universe_size,
                    config,
                );
            }
            Event::AbortConfiguration(epoch) => {
                self.apply_event_abort_configuration(epoch)
            }
            Event::SendNexusReplyOnUnderlay(reply) => {
                self.apply_event_send_nexus_reply_on_underlay(reply)
            }
            Event::DeliverEnvelope(envelope) => {
                self.apply_event_deliver_envelope(envelope);
            }
            Event::LoadRackSecret(id, epoch) => {
                self.apply_event_load_rack_secret(id, epoch);
            }
            Event::ClearSecrets(id) => {
                self.apply_event_clear_secrets(id);
            }
            Event::DeliverNexusReply(reply) => {
                self.apply_event_deliver_nexus_reply(reply);
            }
            Event::CommitConfiguration(dest) => {
                self.apply_event_commit(dest);
            }
            Event::Reconfigure(nexus_config) => {
                self.apply_event_reconfigure(nexus_config)
            }
            Event::LrtqUpgrade(nexus_config) => {
                self.apply_event_lrtq_upgrade(nexus_config)
            }
            Event::CrashNode(id) => {
                self.apply_event_crash_node(id);
            }
            Event::RestartNode { id, connection_order } => {
                self.apply_event_restart_node(id, connection_order);
            }
            Event::PrepareAndCommit(id) => {
                self.apply_event_prepare_and_commit(id);
            }
        }
    }

    fn apply_event_initial_config(
        &mut self,
        member_universe_size: usize,
        config: NexusConfig,
        crashed_nodes: BTreeSet<BaseboardId>,
    ) {
        // Generate the member universe
        self.member_universe = member_universe(member_universe_size);
        // Create the SUT nodes
        self.sut = Sut::new(&self.log, self.member_universe.clone());

        self.crashed_nodes = crashed_nodes;

        // Inform nexus about the initial configuration
        self.nexus.configs.insert_unique(config).expect("new config");

        // Establish bootstrap network connections between live nodes
        for (from, (node, ctx)) in self
            .sut
            .nodes
            .iter_mut()
            .filter(|(id, _)| !self.crashed_nodes.contains(id))
        {
            for to in self
                .member_universe
                .iter()
                .filter(|id| !self.crashed_nodes.contains(id) && from != *id)
            {
                node.on_connect(ctx, to.clone());
            }
        }

        self.send_reconfigure_msg();

        // Check the results of the initial setup
        self.postcondition_initial_configuration();

        // Put the coordinator's outgoing messages on the wire if there are any
        self.send_envelopes_from_coordinator();
    }

    fn apply_event_initial_config_lrtq(
        &mut self,
        member_universe_size: usize,
        config: NexusConfig,
    ) {
        // Generate the member universe
        self.member_universe = member_universe(member_universe_size);

        // Translate `BaseboardId`s to `Baseboards`s for LRTQ membership
        let baseboards: BTreeSet<_> = config
            .members
            .iter()
            .cloned()
            .map(|id| {
                Baseboard::new_pc(
                    id.serial_number.clone(),
                    id.part_number.clone(),
                )
            })
            .collect();

        // Create the LRTQ key share packages and take only the common data,
        // which is what we use for trust quorum upgrade.
        let share_pkgs = config
            .members
            .iter()
            .cloned()
            .zip(
                bootstore::schemes::v0::create_pkgs(
                    self.nexus.rack_id.into_untyped_uuid(),
                    baseboards.clone(),
                )
                .unwrap()
                .expose_secret_mut()
                .iter()
                .map(|pkg| pkg.common.clone()),
            )
            .collect();

        // Create the SUT nodes
        self.sut =
            Sut::new_lrtq(&self.log, self.member_universe.clone(), share_pkgs);

        // Inform nexus about the initial configuration
        self.nexus.configs.insert_unique(config).expect("new config");

        // Establish bootstrap network connections between all nodes
        for (from, (node, ctx)) in self.sut.nodes.iter_mut() {
            for to in self.member_universe.iter().filter(|id| from != *id) {
                node.on_connect(ctx, to.clone());
            }
        }
    }

    fn apply_event_commit(&mut self, id: BaseboardId) {
        let rack_id = self.nexus.rack_id;
        let latest_config = self.nexus.latest_config();
        let (node, ctx) = self.sut.nodes.get_mut(&id).expect("node exists");
        node.commit_configuration(ctx, rack_id, latest_config.epoch)
            .expect("commit succeeded");

        self.underlay_network.push(NexusReply::CommitAck {
            from: id,
            epoch: latest_config.epoch,
        });
    }

    // We only do this at the end of the cluster test in `ensure_liveness`.
    //
    // This means that there must be at least one prepared config that we can
    // use in the `PrepareAndCommit`. We don't pull the config out in test
    // generation because of the lack of RNG seeding, which will result in
    // differences in rack secrets during test execution and tqdb playback. We
    // should fix this eventually, but it probably won't require changing this
    // method.
    fn apply_event_prepare_and_commit(&mut self, id: BaseboardId) {
        // Just grab the configuration from a prepared member here. In real code
        // Nexus will poll the coordinator for a configuration when it polls for
        // prepare acks.
        let prepared = self
            .nexus
            .latest_config()
            .prepared_members
            .first()
            .expect("at least one prepared member");
        let (_, ctx) = self.sut.nodes.get(prepared).expect("node exists");
        let config = ctx
            .persistent_state()
            .latest_config()
            .expect("config exists")
            .clone();
        let (node, ctx) = self.sut.nodes.get_mut(&id).expect("node exists");
        assert!(node.prepare_and_commit(ctx, config).is_ok());
        send_envelopes(ctx, &mut self.bootstrap_network, &self.crashed_nodes);
    }

    fn apply_event_load_rack_secret(&mut self, id: BaseboardId, epoch: Epoch) {
        let (node, ctx) = self.sut.nodes.get_mut(&id).expect("node exists");

        // Postcondition checks
        match node.load_rack_secret(ctx, epoch) {
            Ok(None) => {
                assert!(node.is_collecting_shares_for_rack_secret(epoch));
                // Send any messages output as a result of the load
                send_envelopes(
                    ctx,
                    &mut self.bootstrap_network,
                    &self.crashed_nodes,
                );
            }
            Ok(Some(_)) => {
                // We may be collecting for a later epoch, but haven't thrown
                // out the old secret, so we don't check if we are collecting as
                // in the `Ok(None)` clause above.

                // If we can load a rack secret then we have either committed
                // for this epoch or a later epoch.
                assert!(
                    ctx.persistent_state()
                        .latest_committed_epoch()
                        .expect("at least one committed epoch")
                        >= epoch
                );

                // Send any messages output as a result of the load
                send_envelopes(
                    ctx,
                    &mut self.bootstrap_network,
                    &self.crashed_nodes,
                );
            }
            Err(LoadRackSecretError::NoCommittedConfigurations) => {
                assert!(
                    ctx.persistent_state().latest_committed_epoch().is_none()
                );
            }
            Err(LoadRackSecretError::NotCommitted(epoch)) => {
                assert!(!ctx.persistent_state().commits.contains(&epoch));
            }
            Err(LoadRackSecretError::Alarm) => {
                // We should not see any alarms in this test
                panic!("alarm seen");
            }
            Err(LoadRackSecretError::NotAvailable(_)) => {
                assert!(
                    ctx.persistent_state()
                        .latest_committed_epoch()
                        .expect("latest committed epoch exists")
                        > epoch
                );
            }
        }
    }

    fn apply_event_clear_secrets(&mut self, id: BaseboardId) {
        let (node, _) = self.sut.nodes.get_mut(&id).expect("node exists");
        node.clear_secrets();
    }

    fn apply_event_send_nexus_reply_on_underlay(&mut self, reply: NexusReply) {
        self.underlay_network.push(reply);
    }

    fn apply_event_crash_node(&mut self, id: BaseboardId) {
        // We clear all the crashed node's destination messages
        self.bootstrap_network.remove(&id);

        // Keep track of the crashed node
        self.crashed_nodes.insert(id.clone());

        // We get to define the semantics of the network with regards to an
        // inflight message sourced from a crashed node. We have two choices:
        // drop the message or let it be eventually delivered to the desination
        // if the destination node doesn't crash before delivery. We choose
        // the latter mostly for efficiency: we don't want to have to loop over
        // every destination in the bootstrap network and filter messages.
        //
        // However, we do still have to call `node.on_disconnect()` at all
        // connected nodes, so do that now. For simplicity, we do this at every
        // alive node in the same step.
        for (_, (node, ctx)) in self
            .sut
            .nodes
            .iter_mut()
            .filter(|(id, _)| !self.crashed_nodes.contains(id))
        {
            node.on_disconnect(ctx, id.clone());
        }
    }

    fn apply_event_restart_node(
        &mut self,
        id: BaseboardId,
        connection_order: Vec<BaseboardId>,
    ) {
        // The node is no longer crashed.
        self.crashed_nodes.remove(&id);

        // We need to clear the mutable state of the `Node`. We do this by
        // creating a new `Node` and passing in the existing context which
        // contains the persistent state.
        {
            let (node, ctx) = self.sut.nodes.get_mut(&id).expect("node exists");
            ctx.clear_mutable_state();
            *node = Node::new(&self.log, ctx);
        }

        // We now need to connect to each node in the order given in
        // `connection_order`. We do this by calling `on_connect` at the
        // restarted node and the node in `connection_order`;
        for peer in connection_order {
            let (peer_node, peer_ctx) =
                self.sut.nodes.get_mut(&peer).expect("node exists");
            // Inform the peer of the connection
            peer_node.on_connect(peer_ctx, id.clone());
            // Send any messages output as a result of the connection
            send_envelopes(
                peer_ctx,
                &mut self.bootstrap_network,
                &self.crashed_nodes,
            );

            let (node, ctx) = self.sut.nodes.get_mut(&id).expect("node exists");
            // Inform the restarted node of the connection
            node.on_connect(ctx, peer);
            // Send any messages output as a result of the connection
            send_envelopes(
                ctx,
                &mut self.bootstrap_network,
                &self.crashed_nodes,
            );
        }
    }

    fn apply_event_deliver_nexus_reply(&mut self, recorded_reply: NexusReply) {
        let mut latest_config = self.nexus.latest_config_mut();
        let reply = self.underlay_network.pop().expect("reply exists");
        assert_eq!(recorded_reply, reply);
        match reply {
            NexusReply::AckedPreparesFromCoordinator { epoch, acks } => {
                if epoch == latest_config.epoch {
                    latest_config.prepared_members.extend(acks);

                    if latest_config.can_commit() {
                        drop(latest_config);
                        self.nexus_commit();
                    }
                }
            }
            NexusReply::CommitAck { from, epoch } => {
                if latest_config.epoch == epoch {
                    latest_config.committed_members.insert(from);
                }
            }
        }
    }

    fn apply_event_abort_configuration(&mut self, epoch: Epoch) {
        let mut latest_config = self.nexus.latest_config_mut();
        assert_eq!(epoch, latest_config.epoch);
        latest_config.op = NexusOp::Aborted;
    }

    fn apply_event_deliver_envelope(&mut self, recorded_envelope: Envelope) {
        let envelope = self
            .bootstrap_network
            .get_mut(&recorded_envelope.to)
            .unwrap()
            .pop()
            .expect("envelope in bootstrap network");

        // The recorded envelope must be exactly the same as the one pulled
        // off the bootstrap network. We ignore crypto data because we don't
        // currently seed and track random number generators. For our purposes,
        // validating the other fields is enough, because the test will fail if
        // the crypto doesn't work and decrypt to the same plaintext.
        assert!(recorded_envelope.equal_except_for_crypto_data(&envelope));

        let (node, ctx) =
            self.sut.nodes.get_mut(&envelope.to).expect("destination exists");
        node.handle(ctx, envelope.from, envelope.msg);

        // If this is the first time we've seen a configuration, track it
        //
        // We have to do this here because for reconfigurations, shares
        // for the last committed reconfiguration are gathered before
        // the config is created. We don't know exactly when config
        // generation occurs, but know that it happens after envelopes
        // are delivered, except for configurations that don't have
        // a last committed config. This is normally the initial
        // configuration, but can be later ones if the initial config
        // is aborted.
        if ctx.persistent_state_change_check_and_reset() {
            if let Some(latest_config) = ctx.persistent_state().latest_config()
            {
                if !self
                    .all_coordinated_configs
                    .contains_key(&latest_config.epoch)
                {
                    // The coordinator must be the first node to create
                    // the configuration.
                    assert_eq!(&latest_config.coordinator, ctx.platform_id());

                    self.all_coordinated_configs
                        .insert_unique(latest_config.clone())
                        .expect("unique config");
                }
            }
        }

        // Send any messages as a result of handling this message
        send_envelopes(ctx, &mut self.bootstrap_network, &self.crashed_nodes);

        // Remove any destinations with zero messages in-flight
        self.bootstrap_network.retain(|_, msgs| !msgs.is_empty());
    }

    fn apply_event_reconfigure(&mut self, nexus_config: NexusConfig) {
        self.nexus.configs.insert_unique(nexus_config).expect("new config");
        self.send_reconfigure_msg();
        self.send_envelopes_from_coordinator();
    }

    fn apply_event_lrtq_upgrade(&mut self, nexus_config: NexusConfig) {
        self.nexus.configs.insert_unique(nexus_config).expect("new config");
        self.send_lrtq_upgrade_msg();
        self.send_envelopes_from_coordinator();
    }

    // Commit at nexus when preparing
    fn nexus_commit(&mut self) {
        let mut latest_config = self.nexus.latest_config_mut();
        info!(
            self.log,
            "nexus committed";
            "epoch" => %latest_config.epoch,
            "coordinator" => %latest_config.coordinator
        );

        latest_config.op = NexusOp::Committed;

        let new_members = latest_config.members.clone();
        let new_epoch = latest_config.epoch;

        // Expunge any removed nodes from the last committed configuration
        if let Some(last_committed_epoch) = latest_config.last_committed_epoch {
            // Release our mutable borrow
            drop(latest_config);

            let last_committed_config = self
                .nexus
                .configs
                .get(&last_committed_epoch)
                .expect("config exists");

            let expunged =
                last_committed_config.members.difference(&new_members).cloned();

            for e in expunged {
                info!(
                    self.log,
                    "expunged node";
                    "epoch" => %new_epoch,
                    "platform_id" => %e
                );
                self.expunged.insert(e);
            }
        }
    }
}

/// Broken out of `TqState` to alleviate borrow checker woes
fn send_envelopes(
    ctx: &mut NodeCtx,
    bootstrap_network: &mut BTreeMap<BaseboardId, Vec<Envelope>>,
    crashed_nodes: &BTreeSet<BaseboardId>,
) {
    for envelope in
        ctx.drain_envelopes().filter(|e| !crashed_nodes.contains(&e.to))
    {
        let envelopes =
            bootstrap_network.entry(envelope.to.clone()).or_default();
        envelopes.push(envelope);
    }
}

/// The system under test
///
/// This is our real code.
#[derive(Debug, Clone, Diffable)]
pub struct Sut {
    /// All nodes in the member universe
    pub nodes: BTreeMap<BaseboardId, (Node, NodeCtx)>,
}

impl Sut {
    pub fn empty() -> Sut {
        Sut { nodes: BTreeMap::new() }
    }

    pub fn new(log: &Logger, universe: Vec<BaseboardId>) -> Sut {
        let nodes = universe
            .into_iter()
            .map(|id| {
                let mut ctx = NodeCtx::new(id.clone());
                let node = Node::new(log, &mut ctx);
                (id, (node, ctx))
            })
            .collect();
        Sut { nodes }
    }

    pub fn new_lrtq(
        log: &Logger,
        universe: Vec<BaseboardId>,
        mut share_pkgs: BTreeMap<BaseboardId, SharePkgCommon>,
    ) -> Sut {
        // Populate the persistent state of each member in the LRTQ cluster
        // with a share pkg
        let nodes = universe
            .into_iter()
            .map(|id| {
                let mut ctx = if let Some(pkg) = share_pkgs.remove(&id) {
                    let ps = PersistentState::new_lrtq_only(pkg);
                    NodeCtx::new_with_persistent_state(id.clone(), ps)
                } else {
                    NodeCtx::new(id.clone())
                };
                let node = Node::new(log, &mut ctx);
                (id, (node, ctx))
            })
            .collect();
        Sut { nodes }
    }
}

/*****************************************************************************
 *
 * Diff related display code
 *
 *****************************************************************************/

/// Diff Display functionality for `TqState`
///
/// All diff related code lives in `test-utils`, because we enable the
/// trust-quorum feature `danger_partial_eq_ct_wrapper` in this crate. We
/// don't enable it for all uses of the `trust_quorum` crate, especially in
/// production.
///
/// Since we only use it for human readable output in test tools - at least for
/// now, we put it behind a feature flag and implement all display functionality
/// here.
impl Display for TqStateDiff<'_> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        // The set of SUT nodes never changes
        for (&id, &leaf) in self.sut.nodes.common.iter() {
            if leaf.is_modified() {
                writeln!(f, "Node changed: {id}")?;
                let (node_diff, ctx_diff) = leaf.diff_pair();
                display_node_diff(node_diff, f)?;
                display_node_ctx_diff(ctx_diff, f)?;

                // Add a blank line between modified nodes
                writeln!(f)?;
            }
        }

        display_bootstrap_network_diff(&self.bootstrap_network, f)?;
        display_underlay_network_diff(&self.underlay_network, f)?;
        display_nexus_state_diff(&self.nexus, f)?;
        display_faults_diff(&self.crashed_nodes, f)?;
        display_expunged_diff(&self.expunged, f)?;

        Ok(())
    }
}

fn display_expunged_diff(
    diff: &BTreeSetDiff<'_, BaseboardId>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if !diff.added.is_empty() {
        writeln!(f, "expunged nodes:")?;
        for id in &diff.added {
            writeln!(f, "    {id}")?;
        }
    }
    Ok(())
}

fn display_faults_diff(
    crashed_nodes: &BTreeSetDiff<'_, BaseboardId>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if !crashed_nodes.added.is_empty() {
        writeln!(f, "  Nodes crashed:")?;
        for id in &crashed_nodes.added {
            writeln!(f, "    {id}")?;
        }
    }
    if !crashed_nodes.removed.is_empty() {
        writeln!(f, "  nodes started:")?;
        for id in &crashed_nodes.removed {
            writeln!(f, "    {id}")?;
        }
    }

    Ok(())
}

fn display_nexus_state_diff(
    diff: &NexusStateDiff<'_>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if diff.configs.modified().count() != 0 {
        writeln!(f, "nexus state changed:")?;
    }

    // Nexus configs can only be added or modified
    for c in &diff.configs.added {
        writeln!(f, "    config added at epoch {}, op: {:?}", c.epoch, c.op)?;
    }
    for c in diff.configs.modified_diff() {
        writeln!(f, "    config modified at epoch {}", c.epoch.before)?;
        if c.op.is_modified() {
            let op = c.op.diff_pair();
            writeln!(f, "      op changed: {:?} -> {:?}", op.before, op.after)?;
        }
        for id in c.prepared_members.added {
            writeln!(f, "      new prepare ack received: {id}")?;
        }
        for id in c.committed_members.added {
            writeln!(f, "      new commit ack received: {id}")?;
        }
    }

    Ok(())
}

fn display_underlay_network_diff(
    diff: &Leaf<&[NexusReply]>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if diff.is_unchanged() {
        return Ok(());
    }

    let before: BTreeSet<_> = diff.before.iter().collect();
    let after: BTreeSet<_> = diff.after.iter().collect();

    let added = after.difference(&before).count();
    let removed = before.difference(&after).count();

    writeln!(f, "  {added} new nexus replies in flight on underlay network")?;
    writeln!(
        f,
        "  {removed} nexus replies delivered to nexus from underlay network",
    )?;

    Ok(())
}

fn display_bootstrap_network_diff(
    diff: &BTreeMapDiff<'_, BaseboardId, Vec<Envelope>>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if !diff.added.is_empty() {
        writeln!(f, "  messages newly in flight on bootstrap network:")?;
        for id in diff.added.keys() {
            writeln!(f, "    destination:  {id}")?;
        }
    }

    if !diff.removed.is_empty() {
        writeln!(f, "  all messages delivered from bootstrap network:")?;
        for id in diff.removed.keys() {
            writeln!(f, "    destination:  {id}")?;
        }
    }

    if diff.unchanged_keys().count() != 0 {
        writeln!(f, "  messages remain in flight from bootstrap network:")?;
        for id in diff.unchanged_keys() {
            writeln!(f, "    destination: {id}")?;
        }
    }
    Ok(())
}

// Walk a `NodeCtxDiff` and format it for display
fn display_node_ctx_diff(
    diff: NodeCtxDiff<'_>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if !diff.persistent_state().configs.added.is_empty() {
        writeln!(f, "  config added to persistent state: ")?;
        for c in &diff.persistent_state().configs.added {
            writeln!(f, "    epoch: {}", c.epoch)?;
        }
    }
    if !diff.persistent_state().configs.removed.is_empty() {
        writeln!(f, "  config removed from persistent state: ")?;
        for c in &diff.persistent_state().configs.removed {
            writeln!(f, "    epoch: {}", c.epoch)?;
        }
    }

    if !diff.persistent_state().shares.added.is_empty() {
        writeln!(f, "  our share added to persistent state: ")?;
        for e in diff.persistent_state().shares.added.keys() {
            writeln!(f, "    epoch: {e}")?;
        }
    }
    if !diff.persistent_state().shares.removed.is_empty() {
        writeln!(f, "  our share removed from persistent state: ")?;
        for e in diff.persistent_state().shares.removed.keys() {
            writeln!(f, "    epoch: {e}")?;
        }
    }

    if !diff.persistent_state().commits.added.is_empty() {
        writeln!(f, "  commit added to persistent state: ")?;
        for e in &diff.persistent_state().commits.added {
            writeln!(f, "    epoch: {e}")?;
        }
    }
    if !diff.persistent_state().commits.removed.is_empty() {
        writeln!(f, "  commit removed from persistent state: ")?;
        for e in &diff.persistent_state().commits.removed {
            writeln!(f, "    epoch: {e}")?;
        }
    }

    if diff.outgoing().is_modified() {
        writeln!(f, "  messages sent to or delivered from bootstrap network")?;
    }

    if !diff.connected().added.is_empty() {
        writeln!(f, "  nodes connected:")?;
        for id in &diff.connected().added {
            writeln!(f, "    {id}")?;
        }
    }

    if !diff.connected().removed.is_empty() {
        writeln!(f, "  nodes disconnected:")?;
        for id in &diff.connected().removed {
            writeln!(f, "    {id}")?;
        }
    }

    if !diff.alarms().added.is_empty() {
        writeln!(f, "  alarms triggered:")?;
        for alarm in &diff.alarms().added {
            writeln!(f, "    {alarm:?}")?;
        }
    }

    if !diff.alarms().removed.is_empty() {
        writeln!(f, "  alarms cleared:")?;
        for alarm in &diff.alarms().removed {
            writeln!(f, "    {alarm:?}")?;
        }
    }

    Ok(())
}

// Walk a `NodeDiff` and format it for display
fn display_node_diff(
    node_diff: NodeDiff<'_>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    // Show changes in `Node::coordinator_state`
    if node_diff.coordinator_state().is_modified() {
        writeln!(f, "  coordinator state changed: ")?;
        if node_diff.coordinator_state().before.is_none() {
            writeln!(
                f,
                "    started coordinating at epoch {}",
                node_diff.coordinator_state().after.unwrap().msg().epoch()
            )?;
        } else if node_diff.coordinator_state().after.is_none() {
            writeln!(
                f,
                "    stopped coordinating at epoch {}",
                node_diff.coordinator_state().before.unwrap().msg().epoch()
            )?;
        } else {
            let before = node_diff.coordinator_state().before.unwrap();
            let after = node_diff.coordinator_state().after.unwrap();

            // They are both `Some`, so figure out what changed
            // by recursing
            let diff = before.diff(after);
            display_coordinator_state_diff(diff, f)?;
        }
    }

    // Show changes in `Node::key_share_computer`
    if node_diff.key_share_computer().is_modified() {
        writeln!(f, "  key share computer changed: ")?;
        if node_diff.key_share_computer().before.is_none() {
            writeln!(
                f,
                "    started computing key share at epoch {}",
                node_diff.key_share_computer().after.unwrap().config().epoch
            )?;
        } else if node_diff.key_share_computer().after.is_none() {
            writeln!(
                f,
                "    stopped computing key share at epoch {}",
                node_diff.key_share_computer().before.unwrap().config().epoch
            )?;
        } else {
            writeln!(
                f,
                "    computing key share at epochs: {} -> {}",
                node_diff.key_share_computer().before.unwrap().config().epoch,
                node_diff.key_share_computer().after.unwrap().config().epoch
            )?;
        }
    }

    if node_diff.rack_secret_loader().collector().is_modified() {
        // It's too tedious to do the diff work here right now.
        writeln!(f, "  Rack secret collector changed")?;
    }
    if !node_diff.rack_secret_loader().loaded().added.is_empty() {
        // It's too tedious to do the diff work here right now.
        writeln!(f, "  Rack secrets loaded")?;
    }
    if !node_diff.rack_secret_loader().loaded().removed.is_empty() {
        // It's too tedious to do the diff work here right now.
        writeln!(f, "  Rack secrets cleared")?;
    }

    Ok(())
}

pub fn display_coordinator_state_diff(
    diff: CoordinatorStateDiff<'_>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    match (diff.msg().before, diff.msg().after) {
        (CoordinatingMsg::Reconfig(a), CoordinatingMsg::Reconfig(b)) => {
            display_validated_reconfigure_msg_diff(a.diff(b), f)?;
        }
        (CoordinatingMsg::Upgrade(a), CoordinatingMsg::Upgrade(b)) => {
            display_validated_lrtq_upgrade_msg_diff(a.diff(b), f)?;
        }
        (CoordinatingMsg::Reconfig(_), CoordinatingMsg::Upgrade(_)) => {
            panic!("Cannot go from reconfiguring to LRTQ upgrade");
        }
        (CoordinatingMsg::Upgrade(a), CoordinatingMsg::Reconfig(b)) => {
            writeln!(
                f,
                "    Went from LRTQ upgrade to Reconfig: epoch: {} -> {}",
                a.epoch(),
                b.epoch()
            )?;
        }
    }

    // Configuration contains roughly the same information as a
    // `ValidatedReconfigureMsg`. Let's report the only relevant change.
    if diff.configuration().encrypted_rack_secrets.is_modified() {
        writeln!(f, "    encrypted rack secrets changed")?;
    }

    display_coordinator_operation_diff(diff.op().diff_pair(), f)?;

    Ok(())
}

pub fn display_validated_reconfigure_msg_diff(
    diff: ValidatedReconfigureMsgDiff<'_>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    // diff.rack_id changes when tqdb `rewind` command is used, which makes it
    // confusing. It never changes inside tests, so no need to diff it.
    if diff.epoch().is_modified() {
        writeln!(
            f,
            "    epoch: {} -> {}",
            diff.epoch().before,
            diff.epoch().after
        )?;
    }
    if diff.last_committed_epoch().is_modified() {
        writeln!(
            f,
            "    last committed epoch: {:?} -> {:?}",
            diff.last_committed_epoch().before,
            diff.last_committed_epoch().after
        )?;
    }
    if !diff.members().added.is_empty() {
        writeln!(f, "    added members:")?;
        for member in &diff.members().added {
            writeln!(f, "        {member}")?;
        }
    }
    if !diff.members().removed.is_empty() {
        writeln!(f, "    removed members:")?;
        for member in &diff.members().removed {
            writeln!(f, "        {member}")?;
        }
    }
    if diff.threshold().is_modified() {
        writeln!(
            f,
            "    threshold: {} -> {}",
            diff.threshold().before,
            diff.threshold().after
        )?;
    }
    // Always write out the coordinator id. It's useful for digging.
    writeln!(
        f,
        "    coordinator: {} -> {}",
        diff.coordinator_id().before,
        diff.coordinator_id().after,
    )?;

    Ok(())
}

pub fn display_validated_lrtq_upgrade_msg_diff(
    diff: ValidatedLrtqUpgradeMsgDiff,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    // diff.rack_id changes when tqdb `rewind` command is used, which makes it
    // confusing. It never changes inside tests, so no need to diff it.
    if diff.epoch().is_modified() {
        writeln!(
            f,
            "    epoch: {} -> {}",
            diff.epoch().before,
            diff.epoch().after
        )?;
    }
    if !diff.members().added.is_empty() {
        writeln!(f, "    added members:")?;
        for member in &diff.members().added {
            writeln!(f, "        {member}")?;
        }
    }
    if !diff.members().removed.is_empty() {
        writeln!(f, "    removed members:")?;
        for member in &diff.members().removed {
            writeln!(f, "        {member}")?;
        }
    }
    if diff.threshold().is_modified() {
        writeln!(
            f,
            "    threshold: {} -> {}",
            diff.threshold().before,
            diff.threshold().after
        )?;
    }
    // Always write out the coordinator id. It's useful for digging.
    writeln!(
        f,
        "    coordinator: {} -> {}",
        diff.coordinator_id().before,
        diff.coordinator_id().after,
    )?;

    Ok(())
}

pub fn display_coordinator_operation_diff(
    diff: Leaf<&CoordinatorOperation>,
    f: &mut std::fmt::Formatter<'_>,
) -> std::fmt::Result {
    if diff.is_unchanged() {
        return Ok(());
    }

    // If the same variant changed contents, compare them. Otherwise report the
    // change in variants.
    match (diff.before, diff.after) {
        (
            CoordinatorOperation::CollectShares {
                old_epoch,
                old_collected_shares,
                ..
            },
            CoordinatorOperation::CollectShares {
                old_epoch: after_old_epoch,
                old_collected_shares: after_old_collected_shares,
                ..
            },
        ) => {
            // If the collection epoch changed, then only report that
            if old_epoch != after_old_epoch {
                #[allow(clippy::uninlined_format_args)]
                writeln!(
                    f,
                    "    collecting shares: epoch changed: {} -> {}",
                    old_epoch, after_old_epoch
                )?;
            } else if old_collected_shares != after_old_collected_shares {
                writeln!(
                    f,
                    "    collected shares changed at epoch: {old_epoch}",
                )?;
            }
        }
        (
            CoordinatorOperation::CollectLrtqShares {
                collected_lrtq_shares: before,
                ..
            },
            CoordinatorOperation::CollectLrtqShares {
                collected_lrtq_shares: after,
                ..
            },
        ) => {
            if before != after {
                writeln!(f, "    collected lrtq shares differ")?;
            }
        }
        (
            CoordinatorOperation::Prepare { prepare_acks: before, .. },
            CoordinatorOperation::Prepare { prepare_acks: after, .. },
        ) => {
            if before != after {
                writeln!(f, "    received prepare acks differ")?;
            }
        }
        (before, after) => {
            writeln!(
                f,
                "    coordinator operation changed: {} -> {}",
                before.name(),
                after.name()
            )?;
        }
    }

    Ok(())
}
