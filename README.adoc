:showtitle:
:toc: left
:icons: font

= Oxide Control Plane

This repo houses the Oxide Rack control plane prototype.

image::https://github.com/oxidecomputer/omicron/workflows/Rust/badge.svg[]

== Documentation

Several Oxide RFDs are relevant:

* https://48.rfd.oxide.computer[RFD 48 Control Plane Requirements]
* https://61.rfd.oxide.computer[RFD 61 Control Plane Architecture and Design]
* https://4.rfd.oxide.computer[RFD 4 User Facing API Design]
* https://10.rfd.oxide.computer[RFD 10 API Prototype and Simulated Implementation]

For documentation on the control plane architecture, see https://61.rfd.oxide.computer[RFD 61].

For design and API docs, see the https://rust.docs.corp.oxide.computer/omicron/[generated documentation].  You can generate this yourself with:

[source,text]
----
$ cargo doc --document-private-items
----

Note that `--document-private-items` is configured by default, so you can actually just use `cargo doc`.

== Status

The code here is still rough, but the following are implemented:

* actual API:
** projects: CRUD
** instances: CRUD + boot/halt/reboot (simulated)
** disks: CRD + attach/detach (simulated)
** racks: list + get
** sleds: list + get
* architecture:
** nexus: manages a fleet of Oxide racks
** sled agent: manages a single compute sled in an Oxide fleet
** bootstrap agent: launches sled agent and Nexus.
* basic server infrastructure: configuration, logging
* test coverage for most of the above

Nexus uses CockroachDB for persistent state.  Resources like Instances and Disks are simulated in the sled agent.

See TODO.adoc for more info.

== Build and run

Both normal execution and the test suite expect the prescence of some existing binaries (described below) to be locatable within the `$PATH` environment variable.
However, once these have been provided, the codebase should be buildable with `cargo build`, and testable with `cargo test`.

**Prerequisites:**

. Postgres

Postgres is required to link against libpq for access to the database.

(See https://github.com/oxidecomputer/omicron/issues/213 ; we may be able to remove this dependency long-term)

Linux: `sudo apt-get install libpq-dev`
Mac: `brew install postgresql`
Helios: `pkg install library/postgresql-13`

Additionally, to help all runtime commands find this library, we recommend the following:
[source,text]
----
export LD_LIBRARY_PATH="$(pg_config --libdir)"
----

. CockroachDB v20.2.5.
+
The test suite expects to be able to start a single-node CockroachDB cluster using the `cockroach` executable on your PATH.
On illumos, MacOS, and Linux, you should be able to use the `tools/ci_download_cockroachdb` script to fetch the official CockroachDB binary.  It will be put into `./cockroachdb/bin/cockroach`.
Alternatively, you can follow the https://www.cockroachlabs.com/docs/stable/install-cockroachdb.html[official CockroachDB installation instructions for your platform].

. ClickHouse >= v21.7.1.
+
The test suite expects a running instance of the ClickHouse database server.
The script `./tools/ci_download_clickhouse` can be used to download a pre-built binary for illumos, Linux, or macOS platforms. Once complete, you must manually add the binary (located at `clickhouse/clickhouse`) to your PATH.
You may also install ClickHouse manually; instructions can be found https://clickhouse.tech/docs/en/getting-started/install[here].
See <<_configuring_clickhouse>> for details on ClickHouse's setup and configuration files.
+
. Additional software requirements:
+
On an illumos based machine (Helios, OmniOS), make sure your packages are up to date, and you have the `brand/sparse` package:
+
[source,text]
----
pkg install brand/sparse
pkg install pkg:/package/pkg
pkg update
----
 
You can **format the code** using `cargo fmt`.  Make sure to run this before pushing changes.  The CI checks that the code is correctly formatted.

You can **run the https://github.com/rust-lang/rust-clippy[Clippy linter]** using `cargo clippy \-- -D warnings -A clippy::style`.  Make sure to run this before pushing changes.  The CI checks that the code is clippy-clean.

To **run Omicron** you need to run four programs:

* a CockroachDB cluster.  For development, you can use the `omicron-dev` tool in this repository to start a single-node CockroachDB cluster **that will delete the database when you shut it down.**
* a ClickHouse server. You should use the `omicron-dev` tool for this as well, see below, and as with CockroachDB,
the database files will be deleted when you stop the program.
* `nexus`: the guts of the control plane
* `sled-agent-sim`: a simulator for the component that manages a single sled

The easiest way to start the required databases is to use the built in `omicron-dev` tool.
This tool assumes that the `cockroach` and `clickhouse` executables are on your PATH,
and match the versions above.

1. Start CockroachDB using `omicron-dev db-run`:
+
[source,text]
----
$ cargo run --bin=omicron-dev -- db-run
    Finished dev [unoptimized + debuginfo] target(s) in 0.15s
     Running `target/debug/omicron-dev db-run`
omicron-dev: using temporary directory for database store (cleaned up on clean exit)
omicron-dev: will run this to start CockroachDB:
cockroach start-single-node --insecure --http-addr=:0 --store /var/tmp/omicron_tmp/.tmpM8KpTf/data --listen-addr 127.0.0.1:32221 --listening-url-file /var/tmp/omicron_tmp/.tmpM8KpTf/listen-url
omicron-dev: temporary directory: /var/tmp/omicron_tmp/.tmpM8KpTf
*
* WARNING: ALL SECURITY CONTROLS HAVE BEEN DISABLED!
*
* This mode is intended for non-production testing only.
*
* In this mode:
* - Your cluster is open to any client that can access 127.0.0.1.
* - Intruders with access to your machine or network can observe client-server traffic.
* - Intruders can log in without password and read or write any data in the cluster.
* - Intruders can consume all your server's resources and cause unavailability.
*
*
* INFO: To start a secure server without mandating TLS for clients,
* consider --accept-sql-without-tls instead. For other options, see:
*
* - https://go.crdb.dev/issue-v/53404/v20.2
* - https://www.cockroachlabs.com/docs/v20.2/secure-a-cluster.html
*

omicron-dev: child process: pid 3815
omicron-dev: CockroachDB listening at: postgresql://root@127.0.0.1:32221/omicron?sslmode=disable
omicron-dev: populating database
*
* INFO: Replication was disabled for this cluster.
* When/if adding nodes in the future, update zone configurations to increase the replication factor.
*
CockroachDB node starting at 2021-04-13 15:58:59.680359279 +0000 UTC (took 0.4s)
build:               OSS v20.2.5 @ 2021/03/17 21:00:51 (go1.16.2)
webui:               http://127.0.0.1:41618
sql:                 postgresql://root@127.0.0.1:32221?sslmode=disable
RPC client flags:    cockroach <client cmd> --host=127.0.0.1:32221 --insecure
logs:                /var/tmp/omicron_tmp/.tmpM8KpTf/data/logs
temp dir:            /var/tmp/omicron_tmp/.tmpM8KpTf/data/cockroach-temp022560209
external I/O path:   /var/tmp/omicron_tmp/.tmpM8KpTf/data/extern
store[0]:            path=/var/tmp/omicron_tmp/.tmpM8KpTf/data
storage engine:      pebble
status:              initialized new cluster
clusterID:           8ab646f0-67f0-484d-8010-e4444fb86336
nodeID:              1
omicron-dev: populated database
----
+
Note that as the output indicates, this cluster will be available to anybody that can reach 127.0.0.1.

2. Start the ClickHouse database server:
+
[source,text]
----
$ cargo run --bin omicron-dev -- ch-run
    Finished dev [unoptimized + debuginfo] target(s) in 0.47s
     Running `target/debug/omicron-dev ch-run`
omicron-dev: running ClickHouse (PID: 2463), full command is "clickhouse server --log-file /var/folders/67/2tlym22x1r3d2kwbh84j298w0000gn/T/.tmpJ5nhot/clickhouse-server.log --errorlog-file /var/folders/67/2tlym22x1r3d2kwbh84j298w0000gn/T/.tmpJ5nhot/clickhouse-server.errlog -- --http_port 8123 --path /var/folders/67/2tlym22x1r3d2kwbh84j298w0000gn/T/.tmpJ5nhot"
omicron-dev: using /var/folders/67/2tlym22x1r3d2kwbh84j298w0000gn/T/.tmpJ5nhot for ClickHouse data storage
----

3. `nexus` requires a configuration file to run.  You can use `omicron-nexus/examples/config.toml` to start with.  Build and run it like this:
+
[source,text]
----
$ cargo run --bin=nexus -- omicron-nexus/examples/config.toml
...
listening: http://127.0.0.1:12220
----

4. `sled-agent` only accepts configuration on the command line.  Run it with a uuid identifying itself (this would be a uuid for the sled it's managing), an IP:port for itself, and the IP:port of `nexus`'s _internal_ interface.  Using default config, this might look like this:
+
[source,text]
----
$ cargo run --bin=sled-agent -- run $(uuidgen) 127.0.0.1:12345 127.0.0.1:12221
...
Jun 02 12:21:50.989 INFO listening, local_addr: 127.0.0.1:12345, component: dropshot
----

Once everything is up and running, you can use `curl` directly to hit either of the servers.  But it's easier to use the `oxapi_demo` wrapper (see below).

== Docker image

This repo includes a Dockerfile that builds an image containing the Nexus and sled agent.  There's a GitHub Actions workflow that builds and publishes the Docker image.  This is used by the https://github.com/oxidecomputer/console/[console] project for prototyping, demoing, and testing.  This is **not** the way Omicron will be deployed on production systems, but it's a useful vehicle for working with it.

== Quick demo

There's a small demo tool called `./tools/oxapi_demo` that provides a slightly friendlier interface than `curl`, with the same output format.  To use the demo, the `node` and `json` programs should be installed and available in the users PATH:

[source,text]
----
pkg install node-12
npm i -g json
----

Here's a small demo that creates a project, creates an instance, and attaches a disk to it:

[source,text]
----
$ ./tools/oxapi_demo
oxapi_demo: command not specified

PROJECTS

    projects_list
    project_create_demo    PROJECT_NAME
    project_delete         PROJECT_NAME
    project_get            PROJECT_NAME
    project_list_instances PROJECT_NAME
    project_list_disks     PROJECT_NAME

INSTANCES

    instance_create_demo PROJECT_NAME INSTANCE_NAME
    instance_get         PROJECT_NAME INSTANCE_NAME
    instance_delete      PROJECT_NAME INSTANCE_NAME

    instance_stop        PROJECT_NAME INSTANCE_NAME
    instance_start       PROJECT_NAME INSTANCE_NAME
    instance_reboot      PROJECT_NAME INSTANCE_NAME

    instance_attach_disk PROJECT_NAME INSTANCE_NAME DISK_NAME
    instance_detach_disk PROJECT_NAME INSTANCE_NAME DISK_NAME
    instance_list_disks  PROJECT_NAME INSTANCE_NAME
    instance_get_disk    PROJECT_NAME INSTANCE_NAME DISK_NAME

DISKS

    disk_create_demo PROJECT_NAME DISK_NAME
    disk_get PROJECT_NAME DISK_NAME

HARDWARE

    racks_list
    rack_get     RACK_ID

    sleds_list
    sled_get     SERVER_ID


$ ./tools/oxapi_demo project_create_demo myproject
++ curl -sSi http://127.0.0.1:12220/projects -X POST -T -
++ json -ga
HTTP/1.1 100 Continue

HTTP/1.1 201 Created
content-type: application/json
x-request-id: 6e357d79-dc11-4418-8f9d-3c50fce13027
content-length: 196
date: Wed, 27 May 2020 00:45:03 GMT

{
  "id": "9c159daa-fdd1-402b-bc98-e1bebaeb578c",
  "name": "myproject",
  "description": "a project called myproject",
  "timeCreated": "2020-05-27T00:45:03.563996Z",
  "timeModified": "2020-05-27T00:45:03.563996Z"
}


$ ./tools/oxapi_demo instance_create_demo myproject myinstance
++ curl -sSi http://127.0.0.1:12220/projects/myproject/instances -X POST -T -
++ json -ga
HTTP/1.1 100 Continue

HTTP/1.1 201 Created
content-type: application/json
x-request-id: ec65473d-7c0f-462d-a0cb-2e35fabb6533
content-length: 388
date: Wed, 27 May 2020 00:45:24 GMT

{
  "id": "d58039dc-4ed3-44de-a5d4-cd4696246799",
  "name": "myinstance",
  "description": "an instance called myinstance",
  "timeCreated": "2020-05-27T00:45:24.784729Z",
  "timeModified": "2020-05-27T00:45:24.784729Z",
  "projectId": "9c159daa-fdd1-402b-bc98-e1bebaeb578c",
  "ncpus": 1,
  "memory": 256,
  "bootDiskSize": 1,
  "hostname": "myproject",
  "runState": "starting",
  "timeRunStateUpdated": "2020-05-27T00:45:24.785445Z"
}


$ ./tools/oxapi_demo instance_get myproject myinstance
++ curl -sSi http://127.0.0.1:12220/projects/myproject/instances/myinstance
++ json -ga
HTTP/1.1 200 OK
content-type: application/json
x-request-id: c228154b-ddd1-4758-b51c-e5c0547727ba
content-length: 387
date: Wed, 27 May 2020 00:45:42 GMT

{
  "id": "d58039dc-4ed3-44de-a5d4-cd4696246799",
  "name": "myinstance",
  "description": "an instance called myinstance",
  "timeCreated": "2020-05-27T00:45:24.784729Z",
  "timeModified": "2020-05-27T00:45:24.784729Z",
  "projectId": "9c159daa-fdd1-402b-bc98-e1bebaeb578c",
  "ncpus": 1,
  "memory": 256,
  "bootDiskSize": 1,
  "hostname": "myproject",
  "runState": "running",
  "timeRunStateUpdated": "2020-05-27T00:45:26.291810Z"
}


$ ./tools/oxapi_demo disk_create_demo myproject mydisk
++ curl -sSi http://127.0.0.1:12220/projects/myproject/disks -X POST -T -
++ json -ga
HTTP/1.1 100 Continue

HTTP/1.1 201 Created
content-type: application/json
x-request-id: e004d88f-d8cc-4721-8000-583b504a5372
content-length: 314
date: Wed, 27 May 2020 00:45:34 GMT

{
  "id": "bb225f2a-bba3-4d5c-9161-117b8a5b678d",
  "name": "mydisk",
  "description": "a disk called mydisk",
  "timeCreated": "2020-05-27T00:45:34.119234Z",
  "timeModified": "2020-05-27T00:45:34.119234Z",
  "projectId": "9c159daa-fdd1-402b-bc98-e1bebaeb578c",
  "snapshotId": null,
  "size": 1024,
  "state": "creating",
  "devicePath": "/mnt/mydisk"
}


$ ./tools/oxapi_demo disk_get myproject mydisk
++ curl -sSi http://127.0.0.1:12220/projects/myproject/disks/mydisk
++ json -ga
HTTP/1.1 200 OK
content-type: application/json
x-request-id: 09608040-f645-4f1e-941f-2f4629cdedd0
content-length: 314
date: Wed, 27 May 2020 00:45:50 GMT

{
  "id": "bb225f2a-bba3-4d5c-9161-117b8a5b678d",
  "name": "mydisk",
  "description": "a disk called mydisk",
  "timeCreated": "2020-05-27T00:45:34.119234Z",
  "timeModified": "2020-05-27T00:45:34.119234Z",
  "projectId": "9c159daa-fdd1-402b-bc98-e1bebaeb578c",
  "snapshotId": null,
  "size": 1024,
  "state": "detached",
  "devicePath": "/mnt/mydisk"
}


$ ./tools/oxapi_demo instance_attach_disk myproject myinstance mydisk
++ curl -sSi http://127.0.0.1:12220/projects/myproject/instances/myinstance/disks/mydisk -X PUT -T /dev/null
++ json -ga
HTTP/1.1 201 Created
content-type: application/json
x-request-id: 933cecce-d356-4cd4-b1b2-728cecf29a45
content-length: 214
date: Wed, 27 May 2020 00:46:04 GMT

{
  "instanceName": "myinstance",
  "instanceId": "d58039dc-4ed3-44de-a5d4-cd4696246799",
  "diskName": "mydisk",
  "diskId": "bb225f2a-bba3-4d5c-9161-117b8a5b678d",
  "diskState": {
    "attaching": "d58039dc-4ed3-44de-a5d4-cd4696246799"
  }
}


$ ./tools/oxapi_demo instance_list_disks myproject myinstance 
++ curl -sSi http://127.0.0.1:12220/projects/myproject/instances/myinstance/disks
++ json -ga
HTTP/1.1 200 OK
content-type: application/x-ndjson
x-request-id: fb3f4d2a-4992-4816-94df-1aa2c461777c
content-length: 214
date: Wed, 27 May 2020 00:46:11 GMT

{
  "instanceName": "myinstance",
  "instanceId": "d58039dc-4ed3-44de-a5d4-cd4696246799",
  "diskName": "mydisk",
  "diskId": "bb225f2a-bba3-4d5c-9161-117b8a5b678d",
  "diskState": {
    "attached": "d58039dc-4ed3-44de-a5d4-cd4696246799"
  }
}
----

== Running on Helios

Prerequisite: Have a machine already running Helios. An easy way to
do this is by using a https://github.com/oxidecomputer/helios-engvm[Helios VM].

The control plane repository contains a packaging tool which bundles binaries
and SMF manifests. After building the expected binaries, they can be packaged
in a format which lets them be transferred to a Helios machine.

This tool acts on a `package-manifest.toml` file which describes the packages to be
bundled in the build.

[source,text]
----
$ cargo build
$ ./target/debug/omicron-package package
----

The aforementioned package command fills a target directory of choice
(by default, `out/` within the omicron repository) with tarballs ready
to be unpacked as services.

To install the services on a target machine, the following command
may be executed:

[source,text]
----
# Note that "sudo" is required to install SMF services; an appropriate pfexec
# profile may also be used.
$ sudo ./target/debug/omicron-package install
----

This service installs a bootstrap service, which itself loads other
requested services. The bootstrap service is currently the only
service which is "persistent" across reboots - although it will
initialize other service as part of its setup sequence anyway.

[source,text]
----
# List all services:
$ svcs
# View logs for a service:
$ tail -f $(svcs -L nexus)
----

To uninstall all Omicron services from a machine, the following may be
executed:

[source,text]
----
$ sudo ./target/debug/omicron-package uninstall
----


== Configuration reference

`nexus` requires a TOML configuration file.  There's an example in
`omicron-nexus/examples/config.toml`:

[source,toml]
----
include::omicron-nexus/examples/config.toml[]
----

Supported config properties include:

[cols="1,1,1,3",options="header"]
|===
|Name
|Example
|Required?
|Description

|`database.url`
|`"postgresql://root@127.0.0.1:32221/omicron?sslmode=disable"`
|Yes
|URL identifying the CockroachDB instance(s) to connect to.  CockroachDB is used for all persistent data.

|`dropshot_external`
|
|Yes
|Dropshot configuration for the external server (i.e., the one that operators and developers using the Oxide rack will use).  Specific properties are documented below, but see the Dropshot README for details.

|`dropshot_external.bind_address`
|`"127.0.0.1:12220"`
|Yes
|Specifies that the server should bind to the given IP address and TCP port for the **external** API (i.e., the one that operators and developers using the Oxide rack will use).  In general, servers can bind to more than one IP address and port, but this is not (yet?) supported.

|`dropshot_external.request_body_max_bytes`
|`1000`
|Yes
|Specifies the maximum request body size for the **external** API.

|`dropshot_internal`
|
|Yes
|Dropshot configuration for the internal server (i.e., the one used by the sled agent).  Specific properties are documented below, but see the Dropshot README for details.

|`dropshot_internal.bind_address`
|`"127.0.0.1:12220"`
|Yes
|Specifies that the server should bind to the given IP address and TCP port for the **internal** API (i.e., the one used by the sled agent).  In general, servers can bind to more than one IP address and port, but this is not (yet?) supported.

|`dropshot_internal.request_body_max_bytes`
|`1000`
|Yes
|Specifies the maximum request body size for the **internal** API.

|`id`
|`"e6bff1ff-24fb-49dc-a54e-c6a350cd4d6c"`
|Yes
|Unique identifier for this Nexus instance

|`log`
|
|Yes
|Logging configuration.  Specific properties are documented below, but see the Dropshot README for details.

|`log.mode`
|`"file"`
|Yes
|Controls where server logging will go.  Valid modes are `"stderr-terminal"` and `"file".  If the mode is `"stderr-terminal"`, human-readable output, with colors and other terminal formatting if possible, will be sent to stderr.  If the mode is `"file"`, Bunyan-format output will be sent to the filesystem path given by `log.path`.  See also `log.if_exists`, which controls the behavior if the destination path already exists.

|`log.level`
|`"info"`
|Yes
|Specifies what severity of log messages should be included in the log.  Valid values include `"trace"`, `"debug"`, `"info"`, `"warn"`, `"error"`, and `"critical"`, which are increasing order of severity.  Log messages at the specified level and more severe levels will be included in the log.

|`log.path`
|`"logs/server.log"`
|Only if `log.mode = "file"`
|If `log.mode` is `"file"`, this property determines the path to the log file.
See also `log.if_exists`.

|`log.if_exists`
|`"append"`
|Only if `log.mode = "file"`
|If `log.mode` is `"file"`, this property specifies what to do if the destination log file already exists.  Valid values include `"append"` (which appends to the existing file), `"truncate"` (which truncates the existing file and then uses it as though it had just been created), and `"fail"` (which causes the server to exit immediately with an error).

|===

=== Configuring ClickHouse

The ClickHouse binary uses several sources for its configuration. The binary expects an XML
config file, usually named `config.xml` to be available, or one may be specified with the
`-C` command-line flag. The binary also includes a minimal configuration _embedded_ within
it, which will be used if no configuration file is given or present in the current directory.
The server also accepts command-line flags for overriding the values of the configuration
parameters.

The packages downloaded by `ci_download_clickhouse` include a `config.xml` file with them.
You should probably run ClickHouse via the `omicron-dev` tool, but if you decide to run it
manually, you can start the server with:

[source,text]
$ /path/to/clickhouse server --config-file /path/to/config.xml

The configuration file contains a large number of parameters, but most of them are described
with comments in the included `config.xml`, or you may learn more about them
https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/[here]
and https://clickhouse.tech/docs/en/operations/settings/[here]. Parameters may be updated
in the `config.xml`, and the server will automatically reload them. You may also specify
many of them on the command-line with:

[source,text]
$ /path/to/clickhouse server --config-file /path/to/config.xml -- --param_name param_value ...
