Omicron is a complex piece of software consisting of many build and install-time dependencies. It's
intended to run primarily on illumos based systems, and as such is built to use runtime facilities
of illumos, such as https://illumos.org/man/5/smf[SMF]. Furthermore, Omicron is fundamentally a
distributed system, with its components intended to run on multiple servers communicating over the
network. In order to secure the system, certain cryptographic primitives, such as asymmetric key
pairs and shared secrets are required. Due to the nature of these cryptographic primitives, there is
a requirement for the distribution or creation of files unique to a specific server, such that no
other server has access to those files. Examples of this are private keys, and threshold key
shares, although other non-cryptographic unique files may also become necessary over time.

In order to satisfy the above requirements of building and deploying a complex distributed system
consisting of unique, private files, two CLI tools have been created:

  . link:src/bin/omicron-package.rs[omicron-package] - build, package, install on local machine
  . link:src/bin/thing-flinger.rs[thing-flinger] - build, package, deploy to remote machines


If a user is working on their local illumos based machine, and only wants to run
omicron in single node mode, they should follow the install instruction in
the link:../README.adoc[Omicron README] and use `omicron-package`. If the user
wishes for a more complete workflow, where they can code on their local laptop,
use a remote build machine, and install to multiple machines for a more realistic
deployment, they should use `thing-flinger`.

The remainder of this document will describe a typical workflow for using
thing-flinger, pointing out room for improvement.

== Environment and Configuration


     +------------------+                +------------------+
     |                  |                |                  |
     |                  |                |                  |
     |      Client      |---------------->     Builder      |
     |                  |                |                  |
     |                  |                |                  |
     +------------------+                +------------------+
                                                   |
                                                   |
                                                   |
                                                   |
                       +---------------------------+--------------------------+
                       |                           |                          |
                       |                           |                          |
                       |                           |                          |
              +--------v---------+       +---------v--------+       +---------v--------+
              |                  |       |                  |       |                  |
              |                  |       |                  |       |                  |
              | Deployed Server  |       | Deployed Server  |       | Deployed Server  |
              |                  |       |                  |       |                  |
              |                  |       |                  |       |                  |
              +------------------+       +------------------+       +------------------+


`thing-flinger` defines three types of nodes:

 * Client - Where a user typically edits their code and runs thing-flinger. This can run any OS.
 * Builder - A Helios box where Omicron is built and packaged
 * Deployed Server - Helios machines where Omicron will be installed and run

It's not at all necessary for these to be separate nodes. For example, a client and builder can be
the same machine, as long as it's a Helios box. Same goes for Builder and a deployment server. The
benefit of this separation though, is that it allows editing on something like a laptop, without
having to worry about setting up a development environment on an illumos based host.

Machine topology is configured in a `TOML` file that is passed on the command line. All illumos
machines are listed under `servers`, and just the names are used for configuring a builder and
deployment servers. An link:src/bin/deployment-example.toml[example] is provided.

Thing flinger works over SSH, and so the user must have the public key of their client configured
for their account on all servers. SSH agent forwarding is used to prevent the need for the keys of
the builder to also be on the other servers, thus minimizing needed server configuration.

== Typical Workflow

=== Prerequisites

Ensure you have an account on all illumos boxes, with the client public key in
`~/.ssh/authorized_keys`.

.The build machine must have Rust and cargo installed, as well as
all the dependencies for Omicron installed. Following the *prerequisites* in the
https://github.com/oxidecomputer/omicron/#build-and-run[Build and run] section of the main Omicron
README is probably a good idea.

==== Update `config-rss.toml`

Currently rack setup is driven by a configuration file that lives at
`smf/sled-agent/non-gimlet/config-rss.toml` in the root of this repository. The committed
configuration of that file contains a single `requests` entry (with many
services inside it), which means it will start services on only one sled. To
start services (e.g., nexus) on multiple sleds, add additional entries to that
configuration file before proceeding.

=== Command Based Workflow

==== sync
Copy your source code to the builder.

`+cargo run --bin thing-flinger -- -c <CONFIG> sync+`

==== Install Prerequisites
Install necessary build and runtime dependencies (including downloading prebuilt
binaries like Clickhouse and CockroachDB) on the builder and all deployment
targets. This step only needs to be performed once, absent any changes to the
dependencies, but is idempotent so may be run multiple times.

`+cargo run --bin thing-flinger -- -c <CONFIG> install-prereqs+`

==== check (optional)
Run `cargo check` on the builder against the copy of `omicron` that was sync'd
to it in the previous step.

`+cargo run --bin thing-flinger -- -c <CONFIG> build check+`

==== package
Build and package omicron using `omicron-package` on the builder.

`+cargo run --bin thing-flinger -- -c <CONFIG> build package+`

==== overlay
Create files that are unique to each deployment server.

`+cargo run --bin thing-flinger -- -c <CONFIG> overlay+`

==== install
Install omicron to all machines, in parallel. This consists of copying the packaged omicron tarballs
along with overlay files, and omicron-package and its manifest to a `staging` directory on each
deployment server, and then running omicron-package, installing overlay files, and restarting
services.

`+cargo run --bin thing-flinger -- -c <CONFIG> deploy install+`

==== uninstall
Uninstall omicron from all machines.

`+cargo run --bin thing-flinger -- -c <CONFIG> deploy uninstall+`

=== Current Limitations

`thing-flinger` is an early prototype. It has served so far to demonstrate that unique files,
specifically secret shares, can be created and distributed over ssh, and that omicron can be
installed remotely using `omicron-package`. It is not currently complete enough to fully test a
distributed omicron setup, as the underlying dependencies are not configured yet. Specifically,
`CockroachDB` and perhaps `Clickhouse`, need to be configured to run in multiple server mode. It's
anticipated that the `overlay` feature of `thing-flinger` can be used to generate and distribute
configs for this.

=== Design rationale

`thing-flinger` is a command line program written in rust. It was written this way to build upon
`omicron-package`, which is also in rust, as that is our default language of choice at Oxide.
`thing-flinger` is based around SSH, as that is the minimal viable requirement for a test tool such
as this. Additionally, it provides for the most straightforward implementation, and takes the least
effort to use securely. This particular implementation wraps the openssh ssh client via
`std::process::Command`, rather than using the `ssh2` crate, because ssh2, as a wrapper around
`libssh`, does not support agent-forwarding.

== Notes on Using VMs as Deployed Servers on a Linux Host

TODO: This section should be fleshed out more and potentially lifted to its own
document; for now this is a collection of rough notes.

---

It's possible to use a Linux libvirt host running multiple helios VMs as the
builder/deployment server targets, but it requires some additional setup beyond
`https://github.com/oxidecomputer/helios-engvm[helios-engvm]`.

`thing-flinger` does not have any support for running the
`tools/create_virtual_hardware.sh` script; this will need to be done by hand on
each VM.

---

To enable communication between the VMs over their IPv6 bootstrap networks:

1. Enable IPv6 and DHCP on the virtual network libvirt uses for the VMs; e.g.,

```xml
  <ip family="ipv6" address="fdb0:5254::1" prefix="96">
    <dhcp>
      <range start="fdb0:5254::100" end="fdb0:5254::1ff"/>
    </dhcp>
  </ip>
```

After booting the VMs with this enabled, they should be able to ping each other
over their acquired IPv6 addresses, but connecting to each other over the
`bootstrap6` interface that sled-agent creates will fail.

2. Explicitly add routes in the Linux host for the `bootstrap6` addresses,
specifying the virtual interface libvirt created that is used by the VMs.

```
bash% sudo ip -6 route add fdb0:5254:13:7331::1/64 dev virbr1
bash% sudo ip -6 route add fdb0:5254:f0:acfd::1/64 dev virbr1
```

3. Once the sled-agents advance sufficiently to set up `sled6` interfaces,
routes need to be added for them both in the Linux host and in the Helios VMs.
Assuming two sleds with these interfaces:

```
# VM 1
vioif0/sled6      static   ok           fd00:1122:3344:1::1/64
# VM 2
vioif0/sled6      static   ok           fd00:1122:3344:2::1/64
```

The Linux host needs to be told to route that subnet to the appropriate virtual
interface:

```
bash% ip -6 route add fd00:1122:3344::1/48 dev virbr1
```

and each Helios VM needs to be told to route that subnet to the host gateway:

```
vm% pfexec route add -inet6 fd00:1122:3344::/48 $IPV6_HOST_GATEWAY_ADDR
```
