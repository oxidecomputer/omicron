Some notes on querying

For pagination:

- Timeseries name is enough for paginated list timeseries endpoint.
It's just normal keyset pagination.

- For the timeseries data, we'll be using limit/offset pagination. We'll
run the query to get the consistent timeseries keys each time. This is
the `ScanParams` part of the `WhichPage`. The `PageSelector` is the offset.


Now, how to run more complex queries? A good example is something like,
aggregating the timeseries across all but one field. For example, let's
look at the Nexus HTTP latency data. The fields are:

- name (String)
- id (Uuid)
- route (String)
- method (String)
- status_code (I64)

Imagine we wanted to look at the average latency by route, so averaged
across all methods and status codes. (Let's ingore name/id)

We need to group the timeseries keys by route, to find the set of keys
consistent with each different route. ClickHouse provides the `groupArray`
function, which is an aggregate function that collects multiple values
into an array. So we can do:

```
SELECT
    field_value,
    groupArray(timeseries_key)
FROM fields_string
WHERE field_name = 'route'
GROUP BY field_value;


┌─field_value───────────────────────────────────────────┬─groupArray(timeseries_key)────────────────┐
│ /metrics/producers                                    │ [1916712826069192294,6228796576473532827] │
│ /metrics/collectors                                   │ [1500085842574282480]                     │
│ /metrics/collect/e6bff1ff-24fb-49dc-a54e-c6a350cd4d6c │ [15389669872422126367]                    │
│ /sled_agents/fb0f7546-4d46-40ca-9d56-cbb810684ca7     │ [1166666993114742619]                     │
└───────────────────────────────────────────────────────┴───────────────────────────────────────────┘
```

This gives an array of timeseries keys where the route is each of the values
on the left.

So at a very high level, we can average all the timeseries values where the keys
are in each of these different arrays.


This kinda works. It produces an array of arrays, the counts for each of the
histograms, grouped by the field value.

```
SELECT
    field_value,
    groupArray(counts)
FROM
(
    SELECT
        field_value,
        timeseries_key
    FROM fields_string
    WHERE field_name = 'route'
) AS f0
INNER JOIN
(
    SELECT *
    FROM measurements_histogramf64
) AS meas USING (timeseries_key)
GROUP BY field_value
```

We can extend this `groupArray(bins), groupArray(counts)` to get both.


Ok, we're getting somewhere. The aggregation "combinators" modify the behavior of
aggregations, in pretty suprising and powerful ways. For example:

```
SELECT
    field_value,
    sumForEach(counts)
FROM
(
    SELECT
        field_value,
        timeseries_key
    FROM fields_string
    WHERE field_name = 'route'
) AS f0
INNER JOIN
(
    SELECT *
    FROM measurements_histogramf64
) AS meas USING (timeseries_key)
GROUP BY field_value
```

This applies the `-ForEach` combinator to the sum aggregation. This applies the
aggregation to corresponding elements of a sequence (table?) of arrays. We can
do this with any of the aggregations, `avg`, `min`, etc.


The `-Resample` combinator also looks interesting. It uses its arguments to create
a set of intervals, and applies the aggregation within each of those intervals.
So sort of a group-by interval or window function.

Another useful method is `toStartOfInterval`. This takes a timestamp and an interval,
say 5 seconds, or 10 minutes, and returns the interval into which that timestamp
falls. Could be very helpful for aligning/binning data to time intervals. But
it does "round", in that the bins don't start at the first timestamp, but at
the rounded-down interval from that timestamp.

It's possible to build intervals that start exactly at the first timestamp with:

```
SELECT
    timestamp,
    toStartOfInterval(timestamp, toIntervalMinute(1)) + (
        SELECT toSecond(min(timestamp))
        FROM measurements_histogramf64
    )
FROM measurements_histogramf64
```

Or some other rounding shenanigans.


Putting lots of this together:

```
SELECT
    f0.field_name,
    f0.field_value,
    f1.field_name,
    f1.field_value,
    minForEach(bins),
    avgForEach(counts)
FROM
(
    SELECT
        field_name,
        field_value,
        timeseries_key
    FROM fields_string
    WHERE field_name = 'route'
) AS f0
INNER JOIN
(
    SELECT
        field_name,
        field_value,
        timeseries_key
    FROM fields_i64
    WHERE field_name = 'status_code'
) AS f1 ON f0.timeseries_key = f1.timeseries_key
INNER JOIN
(
    SELECT *
    FROM measurements_histogramf64
) AS meas ON f1.timeseries_key = meas.timeseries_key
GROUP BY
    f0.field_name,
    f0.field_value,
    f1.field_name,
    f1.field_value
```

This selects the field name/value, and the bin and average count for each
histogram, grouping by route and status code.

These inner select statements look similar to the ones we already
implement in `field.as_query`. But in that case we select *, and here we
probably don't want to do that to avoid errors about things not being
in aggregations or group by's.

This works (or is syntactically valid) for scalars, if we replace the
combinators with their non-combinator version: e.g, `avgForEach` -> `avg`.


Other rando thoughts.

It'd be nice to have the query builder be able to handle all these, but
I'm not sure how worth it that is. For example, I don't even think we need
the timeseries keys in this query. For the fields where we are specifying
a condition, we have subqueries like:

```
SELECT *
FROM fields_{TYPE}
WHERE field_name = NAME
AND field_value OP VALUE;
```

For ones where we _don't_ care, we just have the first three lines:
    
```
SELECT *
FROM fields_{TYPE}
WHERE field_name = NAME;
```

We can join successive entries on timeseries keys.

For straight SELECT queries, that's pretty much it, like we have currently.
For AGGREGATION queries, we need to

- Have a group-by for each (field_name, field_value) pair. This is true
even when we're unselective on the field, because we are still taking that
field, and we still need to group the keys accordingly.
- Select the consistent timeseries keys. This is so we can correlate the
results of the aggregation back to the field names/values which we still
get from the key-select query.
- Apply the aggregation to the measurements. For scalars, this just the
aggregation. For histograms, this is the `-Array` or `-ForEach` combinator
for that aggregation, depending on what we're applying.
- ??? to the timestamps? -- some alignment, grouping, subsampling? It seems
this has to come from the aggregation query, because there's not a useful
default.

Speaking of defaults, how do these functions behave with missing data?
Or more subtly, what happens if two histograms (say) have the same number
of bins, but the actual bin edges are different? ClickHouse itself doesn't
deal with this AFAICT, which means we'd need to do that in the client.
Ah, but that is unlikely, since we're only aggregating data from the
same timeseries, with the same key. So far anyway. I'm not sure what'll
happen when we start correlating data between timeseries.
