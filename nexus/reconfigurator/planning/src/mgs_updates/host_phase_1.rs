// This Source Code Form is subject to the terms of the Mozilla Public
// License, v. 2.0. If a copy of the MPL was not distributed with this
// file, You can obtain one at https://mozilla.org/MPL/2.0/.

//! Facilities for making choices about MGS-managed host phase 1 updates

use super::MgsUpdateStatus;
use super::MgsUpdateStatusError;
use crate::mgs_updates::MgsUpdateOutcome;
use nexus_types::deployment::BlueprintArtifactVersion;
use nexus_types::deployment::BlueprintHostPhase2DesiredContents;
use nexus_types::deployment::PendingMgsUpdate;
use nexus_types::deployment::PendingMgsUpdateDetails;
use nexus_types::deployment::PendingMgsUpdateHostPhase1Details;
use nexus_types::deployment::ZoneUnsafeToShutdown;
use nexus_types::deployment::planning_report::FailedHostOsUpdateReason;
use nexus_types::inventory::BaseboardId;
use nexus_types::inventory::Collection;
use nexus_types::inventory::SpType;
use omicron_common::api::external::TufArtifactMeta;
use omicron_common::api::external::TufRepoDescription;
use omicron_common::disk::M2Slot;
use omicron_uuid_kinds::OmicronZoneKind;
use omicron_uuid_kinds::SledUuid;
use omicron_uuid_kinds::TypedUuid;
use slog::Logger;
use slog::debug;
use slog::error;
use std::collections::BTreeMap;
use std::sync::Arc;
use tufaceous_artifact::ArtifactHash;
use tufaceous_artifact::ArtifactKind;

/// Describes a set of blueprint changes to the desired host phase 2 contents
/// for a number of sleds
///
/// This is generated by the planning process whenever it also generates host
/// phase 1 updates.
#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct PendingHostPhase2Changes {
    by_sled: BTreeMap<SledUuid, (M2Slot, BlueprintHostPhase2DesiredContents)>,
}

impl PendingHostPhase2Changes {
    pub(super) fn empty() -> Self {
        Self { by_sled: BTreeMap::new() }
    }

    fn insert(
        &mut self,
        sled_id: SledUuid,
        slot: M2Slot,
        artifact: &TufArtifactMeta,
    ) {
        let contents = BlueprintHostPhase2DesiredContents::Artifact {
            version: BlueprintArtifactVersion::Available {
                version: artifact.id.version.clone(),
            },
            hash: artifact.hash,
        };
        let previous = self.by_sled.insert(sled_id, (slot, contents));
        assert!(
            previous.is_none(),
            "recorded multiple changes for sled {sled_id}"
        );
    }

    pub(super) fn append(&mut self, other: &mut Self) {
        let expected_count = self.by_sled.len() + other.by_sled.len();
        self.by_sled.append(&mut other.by_sled);
        assert_eq!(
            self.by_sled.len(),
            expected_count,
            "appended PendingHostPhase2Changes with duplicate sled IDs"
        );
    }

    pub(super) fn is_empty(&self) -> bool {
        self.by_sled.is_empty()
    }

    pub(crate) fn into_iter(
        self,
    ) -> impl Iterator<Item = (SledUuid, M2Slot, BlueprintHostPhase2DesiredContents)>
    {
        self.by_sled
            .into_iter()
            .map(|(sled_id, (slot, contents))| (sled_id, slot, contents))
    }

    #[cfg(test)]
    pub(crate) fn iter(
        &self,
    ) -> impl Iterator<
        Item = (SledUuid, M2Slot, &BlueprintHostPhase2DesiredContents),
    > + '_ {
        self.by_sled
            .iter()
            .map(|(sled_id, (slot, contents))| (*sled_id, *slot, contents))
    }

    #[cfg(test)]
    pub(super) fn remove(
        &mut self,
        sled_id: &SledUuid,
    ) -> Option<(M2Slot, BlueprintHostPhase2DesiredContents)> {
        self.by_sled.remove(sled_id)
    }

    #[cfg(test)]
    fn len(&self) -> usize {
        self.by_sled.len()
    }
}

/// Determines the status of a pending host phase 1 update by comparing the
/// expected preconditions and postconditions to what's in `inventory`
pub(super) fn update_status(
    baseboard_id: &Arc<BaseboardId>,
    desired_artifact: ArtifactHash,
    inventory: &Collection,
    details: &PendingMgsUpdateHostPhase1Details,
    log: &Logger,
) -> Result<MgsUpdateStatus, MgsUpdateStatusError> {
    let active_phase_1_slot = inventory
        .host_phase_1_active_slot_for(baseboard_id)
        .ok_or_else(|| MgsUpdateStatusError::MissingHostPhase1ActiveSlot)?
        .slot;

    let active_phase_1_hash = inventory
        .host_phase_1_flash_hash_for(active_phase_1_slot, baseboard_id)
        .ok_or_else(|| {
            MgsUpdateStatusError::MissingHostPhase1FlashHash(
                active_phase_1_slot,
            )
        })?
        .hash;

    // Get the latest inventory report from sled-agent; we need this to confirm
    // that it's actually booted the OS image we're trying to update to. If it's
    // not present in inventory at all, we'll assume it's in the process of
    // rebooting and report this as "not done".
    let Some(sled_agent) = inventory.sled_agents.iter().find(|sled_agent| {
        sled_agent.baseboard_id.as_ref() == Some(baseboard_id)
    }) else {
        return Ok(MgsUpdateStatus::NotDone);
    };

    let last_reconciliation =
        sled_agent.last_reconciliation.as_ref().ok_or_else(|| {
            MgsUpdateStatusError::MissingSledAgentLastReconciliation
        })?;
    let boot_disk = *last_reconciliation
        .boot_partitions
        .boot_disk
        .as_ref()
        .map_err(|err| {
            MgsUpdateStatusError::SledAgentErrorDeterminingBootDisk(err.clone())
        })?;

    // If we find the desired artifact in the active slot _and_ we see that
    // sled-agent has successfully booted from that same slot, we're done.
    if active_phase_1_hash == desired_artifact
        && boot_disk == active_phase_1_slot
    {
        return Ok(MgsUpdateStatus::Done);
    }

    // The update hasn't completed. We need to compare the inventory contents
    // (from both the SP and sled-agent) against the expectations we recorded in
    // `details` to check whether the update is still in progress or has become
    // impossible.
    let PendingMgsUpdateHostPhase1Details {
        expected_active_phase_1_slot,
        expected_boot_disk,
        expected_active_phase_1_hash,
        expected_active_phase_2_hash,
        expected_inactive_phase_1_hash,
        // We don't need to check the inactive phase 2 hash at all because
        // there's no way its current value would make this update impossible.
        // This differs from the other "expected" fields. Those are set at
        // planning time to the value found in inventory. They function as a
        // guard that we don't execute an update if anything has changed since
        // we planned the update. As a result, they need to be updated at
        // planning-time if reality has changed. This value is used differently.
        // There's one specific value we expect here for this phase 1 update
        // (namely, the hash of the phase 2 image that goes with it) and a
        // separate process is responsible for seeing this value here and
        // triggering the corresponding phase 2 update. If we checked and found
        // a mismatch here, that would just mean this phase 2 update hasn't
        // happened. If we found a match, that just means the phase 1 update
        // hasn't happened.
        expected_inactive_phase_2_hash: _,
        sled_agent_address,
    } = details;

    // It should be impossible for the sled-agent address to change, unless this
    // sled isn't the one we think it is.
    if sled_agent.sled_agent_address != *sled_agent_address {
        error!(
            log,
            "sled-agent with in-progress MGS-driven update has moved";
            "inventory_sled_agent_address" => %sled_agent.sled_agent_address,
        );
        return Ok(MgsUpdateStatus::Impossible);
    }

    // If the active slot or its contents do not match what we expect, we've
    // changed the active slot _without_ completing the update. It's impossible
    // to proceed.
    if active_phase_1_slot != *expected_active_phase_1_slot
        || active_phase_1_hash != *expected_active_phase_1_hash
    {
        return Ok(MgsUpdateStatus::Impossible);
    }

    // Similarly, if the boot disk or active phase 2 hash does not match what we
    // expect, we've changed boot disks _without_ completing the update.
    if boot_disk != *expected_boot_disk {
        return Ok(MgsUpdateStatus::Impossible);
    }
    let active_phase_2_hash = last_reconciliation
        .boot_partitions
        .slot_details(boot_disk)
        .as_ref()
        .map_err(|err| {
            MgsUpdateStatusError::SledAgentErrorDeterminingBootPartitionDetails {
                slot: boot_disk,
                err: err.clone(),
            }
        })?
        .artifact_hash;
    if active_phase_2_hash != *expected_active_phase_2_hash {
        return Ok(MgsUpdateStatus::Impossible);
    }

    // If the inactive phase 1 hash doesn't match what we expect, we won't be
    // able to pass our preconditions. This one is tricky because we could be in
    // the process of writing to the inactive phase 1 slot, which could cause
    // inventory to fail to collect the hash entirely (fine - we'll return an
    // error and wait to make a decision until inventory didn't fail) or give us
    // a hash that matches a partially-written artifact (which could mean the
    // update is `NotDone` because we're actively writing it). However, we have
    // to treat the latter case as `Impossible`:
    //
    // 1. It's possible we partially wrote the inactive slot and then the MGS
    //    instance sending that update died
    // 2. We can't tell whether we have a hash of a partially-written
    //    `desired_artifact` or a hash of some completely unrelated thing.
    //
    // Returning `Impossible` could cause some unnecessary churn in planning
    // steps, but it should eventually converge.
    let inactive_phase_1_hash = inventory
        .host_phase_1_flash_hash_for(
            active_phase_1_slot.toggled(),
            baseboard_id,
        )
        .ok_or_else(|| {
            MgsUpdateStatusError::MissingHostPhase1FlashHash(
                active_phase_1_slot.toggled(),
            )
        })?
        .hash;
    if inactive_phase_1_hash == *expected_inactive_phase_1_hash {
        Ok(MgsUpdateStatus::NotDone)
    } else {
        Ok(MgsUpdateStatus::Impossible)
    }
}

/// Determine if the given baseboard needs a Host OS update and, if so,
/// returns it. An error means an update is still necessary but cannot be
/// completed.
pub(super) fn try_make_update(
    log: &slog::Logger,
    baseboard_id: &Arc<BaseboardId>,
    inventory: &Collection,
    current_artifacts: &TufRepoDescription,
    unsafe_zone_boards: &BTreeMap<
        Arc<BaseboardId>,
        BTreeMap<TypedUuid<OmicronZoneKind>, ZoneUnsafeToShutdown>,
    >,
) -> Result<MgsUpdateOutcome, FailedHostOsUpdateReason> {
    let Some(sp_info) = inventory.sps.get(baseboard_id) else {
        return Err(FailedHostOsUpdateReason::SpNotInInventory);
    };

    // Only configure host OS updates for sleds.
    //
    // We don't bother logging a return value of `NoUpdateNeeded` for non-sleds,
    // because we will never attempt to configure an update for them (nor should
    // we). For the same reason, we do not return an error.
    match sp_info.sp_type {
        SpType::Sled => (),
        SpType::Power | SpType::Switch => {
            return Ok(MgsUpdateOutcome::NoUpdateNeeded);
        }
    }

    let Some(sled_agent) = inventory.sled_agents.iter().find(|sled_agent| {
        sled_agent.baseboard_id.as_ref() == Some(baseboard_id)
    }) else {
        return Err(FailedHostOsUpdateReason::SledAgentInfoNotInInventory);
    };
    let Some(last_reconciliation) = sled_agent.last_reconciliation.as_ref()
    else {
        return Err(FailedHostOsUpdateReason::LastReconciliationNotInInventory);
    };
    let boot_disk = match &last_reconciliation.boot_partitions.boot_disk {
        Ok(boot_disk) => *boot_disk,
        Err(err) => {
            return Err(FailedHostOsUpdateReason::UnableToDetermineBootDisk(
                err.to_string(),
            ));
        }
    };
    let active_phase_2_hash =
        match &last_reconciliation.boot_partitions.slot_details(boot_disk) {
            Ok(details) => details.artifact_hash,
            Err(err) => {
                return Err(
                FailedHostOsUpdateReason::UnableToRetrieveBootDiskPhase2Image(
                    err.to_string(),
                ),
            );
            }
        };

    let Some(active_phase_1_slot) =
        inventory.host_phase_1_active_slot_for(baseboard_id).map(|s| s.slot)
    else {
        return Err(
            FailedHostOsUpdateReason::ActiveHostPhase1SlotNotInInventory,
        );
    };

    // TODO-correctness What should we do if the active phase 1 slot doesn't
    // match the boot disk? That means the active phase 1 slot has been changed
    // since the last time the sled booted, which should only happen at the very
    // end of a host OS update just before the sled is rebooted. It's possible
    // (albeit unlikely) we collected inventory in that window; we don't want to
    // plan a new update for this sled if it's about to reboot into some other
    // update.
    //
    // If there are other ways we could get a mismatch between the active phase
    // 1 slot and the boot disk, they'll induce a support case to recover, given
    // this current implementation. As far as we know they shouldn't happen.
    if active_phase_1_slot != boot_disk {
        return Err(
            FailedHostOsUpdateReason::ActiveHostPhase1SlotBootDiskMismatch(
                active_phase_1_slot,
            ),
        );
    }

    let Some(active_phase_1_hash) = inventory
        .host_phase_1_flash_hash_for(active_phase_1_slot, baseboard_id)
        .map(|h| h.hash)
    else {
        return Err(
            FailedHostOsUpdateReason::ActiveHostPhase1HashNotInInventory(
                active_phase_1_slot,
            ),
        );
    };

    let Some(inactive_phase_1_hash) = inventory
        .host_phase_1_flash_hash_for(
            active_phase_1_slot.toggled(),
            baseboard_id,
        )
        .map(|h| h.hash)
    else {
        return Err(
            FailedHostOsUpdateReason::InactiveHostPhase1HashNotInInventory(
                active_phase_1_slot.toggled(),
            ),
        );
    };

    let mut phase_1_artifacts = Vec::with_capacity(1);
    let mut phase_2_artifacts = Vec::with_capacity(1);
    for artifact in &current_artifacts.artifacts {
        // TODO-correctness we only support gimlet at the moment, need
        // to tell if this target is a gimlet or a comso
        if artifact.id.kind == ArtifactKind::GIMLET_HOST_PHASE_1 {
            phase_1_artifacts.push(artifact);
        } else if artifact.id.kind == ArtifactKind::HOST_PHASE_2 {
            phase_2_artifacts.push(artifact);
        }
    }
    let (phase_1_artifact, phase_2_artifact) =
        match (phase_1_artifacts.as_slice(), phase_2_artifacts.as_slice()) {
            // Common case: Exactly 1 of each artifact.
            ([p1], [p2]) => (p1, p2),
            // "TUF is broken" cases: missing both, one or the other.
            ([], []) => {
                return Err(FailedHostOsUpdateReason::NoMatchingArtifactsFound);
            }
            ([], _) => {
                return Err(
                    FailedHostOsUpdateReason::NoMatchingPhase1ArtifactFound,
                );
            }
            (_, []) => {
                return Err(
                    FailedHostOsUpdateReason::NoMatchingPhase2ArtifactFound,
                );
            }
            // "TUF is broken" cases: have multiple of one or the other. This
            // should be impossible unless we shipped a TUF repo with multiple
            // host OS images. We can't proceed, because we don't know how to
            // pair up which phase 1 matches which phase 2.
            (_, _) => {
                return Err(FailedHostOsUpdateReason::TooManyMatchingArtifacts);
            }
        };

    // If the artifact matches what's deployed, then no update is needed. We
    // only need to look at the running phase 2; that tells us what we're
    // _actually_ running. The currently-active phase 1 should certainly match
    // it; what should we do if it's different? (That should be impossible! It
    // would mean the active phase 1 contents have changed in such a way that
    // this sled will fail to boot if it were rebooted now.)
    if active_phase_2_hash == phase_2_artifact.hash {
        debug!(log, "no host OS update needed for board"; baseboard_id);
        return Ok(MgsUpdateOutcome::NoUpdateNeeded);
    }

    // Make sure the board we're targetting doesn't contain any zones that are
    // unsafe to shut down
    if let Some(unsafe_zone_board) = unsafe_zone_boards.get(baseboard_id) {
        if unsafe_zone_board.is_empty() {
            // This should never happen! If it does, we have a bug somewhere
            // else: we added an entry to `unsafe_zone_boards` but didn't
            // populate the details of why.
            let err = "unsafe zone present, but no details found \
                 (this is unexpected!)"
                .to_string();
            return Err(FailedHostOsUpdateReason::UnsafeZoneFound(err));
        }

        let mut unsafe_zones = Vec::new();
        for (zone_id, zone) in unsafe_zone_board {
            unsafe_zones.push(format!("{}: {}", zone_id, zone));
        }
        let zone_str = unsafe_zones.join(", ").to_string();
        return Err(FailedHostOsUpdateReason::UnsafeZoneFound(zone_str));
    }

    // Before we can proceed with the phase 1 update, we need sled-agent to
    // write the corresponding phase 2 artifact to its inactive disk. This
    // requires us updating its `OmicronSledConfig`. We don't thread the
    // blueprint editor all the way down to this point, so instead we'll return
    // the set of host phase 2 changes we want the planner to make on our
    // behalf.
    let mut pending_host_phase_2_changes = PendingHostPhase2Changes::empty();
    pending_host_phase_2_changes.insert(
        sled_agent.sled_id,
        boot_disk.toggled(),
        phase_2_artifact,
    );

    Ok(MgsUpdateOutcome::Pending(
        PendingMgsUpdate {
            baseboard_id: baseboard_id.clone(),
            sp_type: sp_info.sp_type,
            slot_id: sp_info.sp_slot,
            details: PendingMgsUpdateDetails::HostPhase1(
                PendingMgsUpdateHostPhase1Details {
                    expected_active_phase_1_slot: active_phase_1_slot,
                    expected_boot_disk: boot_disk,
                    expected_active_phase_1_hash: active_phase_1_hash,
                    expected_active_phase_2_hash: active_phase_2_hash,
                    expected_inactive_phase_1_hash: inactive_phase_1_hash,
                    expected_inactive_phase_2_hash: phase_2_artifact.hash,
                    sled_agent_address: sled_agent.sled_agent_address,
                },
            ),
            artifact_hash: phase_1_artifact.hash,
            artifact_version: phase_1_artifact.id.version.clone(),
        },
        pending_host_phase_2_changes,
    ))
}

#[cfg(test)]
mod tests {
    use crate::mgs_updates::ImpossibleUpdatePolicy;
    use crate::mgs_updates::MgsUpdatePlanner;
    use crate::mgs_updates::test_helpers::ARTIFACT_HASH_HOST_PHASE_1;
    use crate::mgs_updates::test_helpers::ARTIFACT_HASH_HOST_PHASE_1_V1;
    use crate::mgs_updates::test_helpers::ARTIFACT_HASH_HOST_PHASE_1_V1_5;
    use crate::mgs_updates::test_helpers::ARTIFACT_HASH_HOST_PHASE_2;
    use crate::mgs_updates::test_helpers::ARTIFACT_HASH_HOST_PHASE_2_V1;
    use crate::mgs_updates::test_helpers::ARTIFACT_VERSION_2;
    use crate::mgs_updates::test_helpers::TestBoards;
    use dropshot::ConfigLogging;
    use dropshot::ConfigLoggingLevel;
    use dropshot::test_util::LogContext;
    use nexus_types::deployment::BlueprintArtifactVersion;
    use nexus_types::deployment::BlueprintHostPhase2DesiredContents;
    use nexus_types::deployment::PendingMgsUpdateDetails;
    use nexus_types::deployment::PendingMgsUpdateHostPhase1Details;
    use nexus_types::deployment::PendingMgsUpdates;
    use nexus_types::deployment::TargetReleaseDescription;
    use nexus_types::inventory::SpType;
    use omicron_common::disk::M2Slot;
    use std::collections::BTreeMap;
    use std::collections::BTreeSet;

    // Short hand-rolled update sequence that exercises some basic behavior for
    // host OS updates.
    #[test]
    fn test_basic_host_os() {
        let test_name = "planning_mgs_updates_basic_host_os";
        let logctx = LogContext::new(
            test_name,
            &ConfigLogging::StderrTerminal { level: ConfigLoggingLevel::Debug },
        );
        let log = &logctx.log;
        let test_boards = TestBoards::new(test_name);

        // Test that with no updates pending and no TUF repo specified, there
        // will remain no updates pending.
        let collection = test_boards
            .collection_builder()
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let current_boards = &collection.baseboards;
        let sled_0_id = test_boards.sled_id(0).expect("have sled 0");
        let sled_1_id = test_boards.sled_id(1).expect("have sled 1");
        let initial_updates = PendingMgsUpdates::new();
        let nmax_updates = 1;
        let impossible_update_policy = ImpossibleUpdatePolicy::Reevaluate;
        let planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &initial_updates,
            current_artifacts: &TargetReleaseDescription::Initial,
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert!(planned.pending_updates.is_empty());
        assert!(planned.pending_host_phase_2_changes.is_empty());

        // Test that when a TUF repo is specified and one host OS is outdated,
        // then it's configured with an update (and the update looks correct).
        let repo = test_boards.tuf_repo();
        let planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &initial_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert_eq!(planned.pending_updates.len(), 1);
        let first_update =
            planned.pending_updates.iter().next().expect("at least one update");
        assert_eq!(first_update.baseboard_id.serial_number, "sled_0");
        assert_eq!(first_update.sp_type, SpType::Sled);
        assert_eq!(first_update.slot_id, 0);
        assert_eq!(first_update.artifact_hash, ARTIFACT_HASH_HOST_PHASE_1);
        assert_eq!(first_update.artifact_version, ARTIFACT_VERSION_2);
        assert_eq!(planned.pending_host_phase_2_changes.len(), 1);
        let (phase2_id, phase2_slot, phase2_contents) =
            planned.pending_host_phase_2_changes.iter().next().unwrap();
        assert_eq!(phase2_id, sled_0_id);
        assert_eq!(phase2_slot, M2Slot::B);
        assert_eq!(
            *phase2_contents,
            BlueprintHostPhase2DesiredContents::Artifact {
                version: BlueprintArtifactVersion::Available {
                    version: ARTIFACT_VERSION_2
                },
                hash: ARTIFACT_HASH_HOST_PHASE_2
            }
        );

        // Test that when an update is already pending, and nothing changes
        // about the state of the world (i.e., the inventory), then the planner
        // makes no changes.
        let later_planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        // We should keep the pending MGS update, but not return any new phase 2
        // changes. (Those had already been applied to the sled configs.)
        assert_eq!(planned.pending_updates, later_planned.pending_updates);
        assert!(later_planned.pending_host_phase_2_changes.is_empty());

        // Test that when two updates are needed, but one is already pending,
        // then the other one is *not* started (because it exceeds
        // nmax_updates).
        let later_collection = test_boards
            .collection_builder()
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .host_active_exception(
                1,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let later_planned = MgsUpdatePlanner {
            log,
            inventory: &later_collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert_eq!(planned.pending_updates, later_planned.pending_updates);
        assert!(later_planned.pending_host_phase_2_changes.is_empty());

        // At this point, we're ready to test that when the first update
        // completes, then the second one *is* started.  This tests two
        // different things: first that we noticed the first one completed, and
        // second that we noticed another thing needed an update
        let later_collection = test_boards
            .collection_builder()
            .host_active_exception(
                1,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let later_planned = MgsUpdatePlanner {
            log,
            inventory: &later_collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert_eq!(later_planned.pending_updates.len(), 1);
        let first_update = later_planned
            .pending_updates
            .iter()
            .next()
            .expect("at least one update");
        assert_eq!(first_update.baseboard_id.serial_number, "sled_1");
        assert_eq!(first_update.sp_type, SpType::Sled);
        assert_eq!(first_update.slot_id, 1);
        assert_eq!(first_update.artifact_hash, ARTIFACT_HASH_HOST_PHASE_1);
        assert_eq!(first_update.artifact_version, ARTIFACT_VERSION_2);
        assert_eq!(later_planned.pending_host_phase_2_changes.len(), 1);
        let (phase2_id, phase2_slot, phase2_contents) =
            later_planned.pending_host_phase_2_changes.iter().next().unwrap();
        assert_eq!(phase2_id, sled_1_id);
        assert_eq!(phase2_slot, M2Slot::B);
        assert_eq!(
            *phase2_contents,
            BlueprintHostPhase2DesiredContents::Artifact {
                version: BlueprintArtifactVersion::Available {
                    version: ARTIFACT_VERSION_2
                },
                hash: ARTIFACT_HASH_HOST_PHASE_2
            }
        );

        // Finally, test that when all OSs are in spec, then no updates are
        // configured.
        let updated_collection = test_boards.collection_builder().build();
        let later_planned = MgsUpdatePlanner {
            log,
            inventory: &updated_collection,
            current_boards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &later_planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert!(later_planned.pending_updates.is_empty());
        assert!(later_planned.pending_host_phase_2_changes.is_empty());

        // Test that we don't try to update boards that aren't in
        // `current_boards`, even if they're in inventory and outdated.
        let collection = test_boards
            .collection_builder()
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &BTreeSet::new(),
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &PendingMgsUpdates::new(),
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert!(planned.pending_updates.is_empty());
        assert!(planned.pending_host_phase_2_changes.is_empty());
        let planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &collection.baseboards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &PendingMgsUpdates::new(),
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        // We verified most of the details above.  Here we're just double
        // checking that the baseboard being missing is the only reason that no
        // update was generated.
        assert_eq!(planned.pending_updates.len(), 1);
        assert_eq!(planned.pending_host_phase_2_changes.len(), 1);

        // Verify the precondition details of an ordinary update.
        let old_update = planned
            .pending_updates
            .into_iter()
            .next()
            .expect("at least one update");
        let PendingMgsUpdateDetails::HostPhase1(
            PendingMgsUpdateHostPhase1Details {
                expected_active_phase_1_slot,
                expected_boot_disk,
                expected_active_phase_1_hash,
                expected_active_phase_2_hash,
                expected_inactive_phase_1_hash,
                expected_inactive_phase_2_hash,
                sled_agent_address: _,
            },
        ) = &old_update.details
        else {
            panic!("expected host phase 1 update");
        };
        assert_eq!(M2Slot::A, *expected_active_phase_1_slot);
        assert_eq!(M2Slot::A, *expected_boot_disk);
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1,
            *expected_active_phase_1_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_2_V1,
            *expected_active_phase_2_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1,
            *expected_inactive_phase_1_hash
        );
        // Note: Not V1! This should be the _new_ artifact hash.
        assert_eq!(ARTIFACT_HASH_HOST_PHASE_2, *expected_inactive_phase_2_hash);

        // Test that if the inactive slot contents have changed, then we'll get
        // a new update reflecting that.
        let collection = test_boards
            .collection_builder()
            .host_phase_1_artifacts(
                ARTIFACT_HASH_HOST_PHASE_1,
                ARTIFACT_HASH_HOST_PHASE_1_V1_5,
            )
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let new_planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &collection.baseboards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert_ne!(planned.pending_updates, new_planned.pending_updates);
        assert_eq!(new_planned.pending_updates.len(), 1);
        let new_update = new_planned
            .pending_updates
            .into_iter()
            .next()
            .expect("at least one update");
        assert_eq!(old_update.baseboard_id, new_update.baseboard_id);
        assert_eq!(old_update.sp_type, new_update.sp_type);
        assert_eq!(old_update.slot_id, new_update.slot_id);
        assert_eq!(old_update.artifact_hash, new_update.artifact_hash);
        assert_eq!(old_update.artifact_version, new_update.artifact_version);
        let PendingMgsUpdateDetails::HostPhase1(
            PendingMgsUpdateHostPhase1Details {
                expected_active_phase_1_slot,
                expected_boot_disk,
                expected_active_phase_1_hash,
                expected_active_phase_2_hash,
                expected_inactive_phase_1_hash,
                expected_inactive_phase_2_hash,
                sled_agent_address: _,
            },
        ) = &new_update.details
        else {
            panic!("expected host phase 1 update");
        };
        assert_eq!(M2Slot::A, *expected_active_phase_1_slot);
        assert_eq!(M2Slot::A, *expected_boot_disk);
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1,
            *expected_active_phase_1_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_2_V1,
            *expected_active_phase_2_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1_5,
            *expected_inactive_phase_1_hash
        );
        assert_eq!(ARTIFACT_HASH_HOST_PHASE_2, *expected_inactive_phase_2_hash);

        // Test that if instead it's the active slot whose contents have changed
        // to something other than the new expected version, then we'll also get
        // a new update reflecting that.
        let collection = test_boards
            .collection_builder()
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1_5,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let new_planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &collection.baseboards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert_ne!(planned.pending_updates, new_planned.pending_updates);
        assert_eq!(new_planned.pending_updates.len(), 1);
        let new_update = new_planned
            .pending_updates
            .into_iter()
            .next()
            .expect("at least one update");
        assert_eq!(old_update.baseboard_id, new_update.baseboard_id);
        assert_eq!(old_update.sp_type, new_update.sp_type);
        assert_eq!(old_update.slot_id, new_update.slot_id);
        assert_eq!(old_update.artifact_hash, new_update.artifact_hash);
        assert_eq!(old_update.artifact_version, new_update.artifact_version);
        let PendingMgsUpdateDetails::HostPhase1(
            PendingMgsUpdateHostPhase1Details {
                expected_active_phase_1_slot,
                expected_boot_disk,
                expected_active_phase_1_hash,
                expected_active_phase_2_hash,
                expected_inactive_phase_1_hash,
                expected_inactive_phase_2_hash,
                sled_agent_address: _,
            },
        ) = &new_update.details
        else {
            panic!("expected host phase 1 update");
        };
        assert_eq!(M2Slot::A, *expected_active_phase_1_slot);
        assert_eq!(M2Slot::A, *expected_boot_disk);
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1_5,
            *expected_active_phase_1_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_2_V1,
            *expected_active_phase_2_hash
        );
        assert_eq!(
            ARTIFACT_HASH_HOST_PHASE_1_V1,
            *expected_inactive_phase_1_hash
        );
        assert_eq!(ARTIFACT_HASH_HOST_PHASE_2, *expected_inactive_phase_2_hash);

        logctx.cleanup_successful();
    }

    // Tests the case where a sled appears to move while a host OS update is
    // pending
    #[test]
    fn test_sled_move() {
        let test_name = "planning_mgs_updates_sled_move";
        let logctx = LogContext::new(
            test_name,
            &ConfigLogging::StderrTerminal { level: ConfigLoggingLevel::Debug },
        );
        let test_boards = TestBoards::new(test_name);

        // Configure an update for one SP.
        let log = &logctx.log;
        let repo = test_boards.tuf_repo();
        let mut collection = test_boards
            .collection_builder()
            .host_active_exception(
                0,
                ARTIFACT_HASH_HOST_PHASE_1_V1,
                ARTIFACT_HASH_HOST_PHASE_2_V1,
            )
            .build();
        let nmax_updates = 1;
        let impossible_update_policy = ImpossibleUpdatePolicy::Reevaluate;
        let planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &collection.baseboards.clone(),
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &PendingMgsUpdates::new(),
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert!(!planned.pending_updates.is_empty());
        assert!(!planned.pending_host_phase_2_changes.is_empty());
        let update = planned
            .pending_updates
            .into_iter()
            .next()
            .expect("at least one update");

        // Move an SP (as if someone had moved the sled to a different cubby).
        // This is awful, but at least it's easy.
        let sp_info = collection
            .sps
            .values_mut()
            .find(|sp| sp.sp_type == SpType::Sled && sp.sp_slot == 0)
            .expect("missing sled 0 SP");
        sp_info.sp_slot = 9;

        // Plan again.  The configured update should be updated to reflect the
        // new location.
        let new_planned = MgsUpdatePlanner {
            log,
            inventory: &collection,
            current_boards: &collection.baseboards,
            unsafe_zone_boards: &BTreeMap::new(),
            current_updates: &planned.pending_updates,
            current_artifacts: &TargetReleaseDescription::TufRepo(repo.clone()),
            nmax_updates,
            impossible_update_policy,
        }
        .plan();
        assert!(!new_planned.pending_updates.is_empty());
        assert!(!new_planned.pending_host_phase_2_changes.is_empty());
        let new_update = new_planned
            .pending_updates
            .into_iter()
            .next()
            .expect("at least one update");
        assert_eq!(new_update.slot_id, 9);
        assert_eq!(new_update.baseboard_id, update.baseboard_id);
        assert_eq!(new_update.sp_type, update.sp_type);
        assert_eq!(new_update.artifact_hash, update.artifact_hash);
        assert_eq!(new_update.artifact_version, update.artifact_version);
        assert_eq!(new_update.details, update.details);

        logctx.cleanup_successful();
    }
}
