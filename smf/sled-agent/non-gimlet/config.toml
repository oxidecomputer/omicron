# Sled Agent Configuration

# Identifies whether sled agent treats itself as a scrimlet or a gimlet.
#
# If this is set to "scrimlet", the sled agent treats itself as a scrimlet.
# If this is set to "gimlet", the sled agent treats itself as a gimlet.
# If this is set to "auto":
# - On illumos, the sled automatically detects whether or not it is a scrimlet.
# - On all other platforms, the sled assumes it is a gimlet.
sled_mode = "scrimlet"

# Identifies the revision of the sidecar that is attached, if one is attached.
# TODO: This field should be removed once Gimlets have the ability to auto-detect
# this information.
sidecar_revision.soft_zone = { front_port_count = 1, rear_port_count = 1 }

# Setting this to true causes sled-agent to always report that its time is
# in-sync, rather than querying its NTP zone.
skip_timesync = false

# For testing purposes, a file-backed virtual disk can be manually created with
# the following:
#
# # truncate -s 10GB <name>.vdev
#
# Note that you'll need to create one such file for each disk below.
# `cargo xtask virtual-hardware create` does this for you.
#
# These paths have the prefix of either "u2" or "m2", followed by an underscore,
# followed by a string that is embedded into their fake serial values.
vdevs = [
  "m2_0.vdev",
  "m2_1.vdev",

  "u2_0.vdev",
  "u2_1.vdev",
  "u2_2.vdev",
  "u2_3.vdev",
  "u2_4.vdev",
  "u2_5.vdev",
  "u2_6.vdev",
  "u2_7.vdev",
  "u2_8.vdev",
]

# The amount of memory held back for services which exist between zero and one
# times on this system. As this is a non-Gimlet target, it is likely that
# services will all cohabitate here, so this is perhaps less overallocation than
# it would be for a Gimlet. This currently includes some additional terms
# reflecting OS memory use under load.
#
# These values are inspired by Gimlet configs but scaled to expected test
# system size - 10gbit network links, 64-128gb of DRAM. Even so, this is likely
# an overestimate:
# * Network buffer slush: 2 GiB
# * Other kernel heap: 2 GiB
# * ZFS ARC minimum: 1 GiB
# * Sled agent: 0.5 GiB
# * Maghemite: 0.25 GiB
# * NTP: 0.25 GiB
control_plane_memory_earmark_mb = 6144

# Percentage of otherwise-unbudgeted physical DRAM to use for the VMM reservoir,
# which guest memory is pulled from.
#
# Unlike the Gimlet configurations, this number is not described by an RFD, but
# has been chosen to not affect development workflows on expected test hardware..
# The example BOMs describe workstations with 128 GiB of memory, where the
# initial strategy had sized the VMM reservoir at 64 GiB. A smaller 64 GiB
# workstation had its VMM reservoir sized to 32 GiB. Taking the next
# multiple of 10 percentage is an arbitrary choice that, for these two shapes of
# system, gives the following reservoir sizes:
# * 128 - 6 (earmark) - 3.75 (`page_t`s)) * 0.6 == 70.95 GiB VMM reservoir
# * 64 - 6 (earmark) - 1.875 (`page_t`s)) * 0.6 == 33.675 GiB VMM reservoir
#
# or reservoirs just a bit larger than their previous 64 and 32 GiB values.
vmm_reservoir_percentage = 25

# Optionally you can specify the size of the VMM reservoir in MiB.
# Note vmm_reservoir_percentage and vmm_reservoir_size_mb cannot be specified
# at the same time.
#vmm_reservoir_size_mb = 2048

# Swap device size for the system. The device is a sparsely allocated zvol on
# the internal zpool of the M.2 that we booted from.
#
# If use of the VMM reservoir is configured, it is likely the system will not
# work without a swap device configured.
swap_device_size_gb = 64

# An optional data link from which we extract a MAC address.
# This is used as a unique identifier for the bootstrap address.
#
# If empty, this will be equivalent to the first result from:
# $ dladm show-phys -p -o LINK
# data_link = "igb0"

# On a multi-sled system, transit-mode Maghemite runs in the `oxz_switch` zone
# to configure routes between sleds.  This runs over the Sidecar's rear ports
# (whether simulated with SoftNPU or not).  On a Gimlet deployed in a rack,
# tfportd will create the necessary links and Maghemite will be configured to
# use those.  But on non-Gimlet systems, you need to specify physical links to
# be passed into the `oxz_switch` zone for this purpose.  You can skip this if
# you're deploying a single-sled system and just leave the single tfportrear
# as-is.
switch_zone_maghemite_links = ["tfportrear0_0"]

data_links = ["net0", "net1"]

[dropshot]
default_request_body_max_bytes = 1048576

[log]
level = "debug"
mode = "file"
path = "/dev/stdout"
if_exists = "append"

# These are pre-generated keys and roots designed to be used for testing.
# Create your own if you need to test a multi-node system
# See the .kdl file for use with pki-playground for generating
[sprockets]
resolve = { which = "local", priv_key = "/opt/oxide/sled-agent/pkg/sprockets-auth.key.pem", cert_chain = "/opt/oxide/sled-agent/pkg/sprockets-chain.pem" }
roots = ["/opt/oxide/sled-agent/pkg/root.cert.pem"]
