:showtitle:
:toc: left
:icons: font

= Running Omicron (Non-Simulated)

Omicron is the control plane for an Oxide rack. It expects to execute
on Helios systems, and Sleds use Helios-specific interfaces to manage
resources.

If you're interested in running the control plane on other platforms, including
Linux and Mac, refer to the guide on xref:how-to-run-simulated.adoc[running
simulated Omicron].

== Installing Prerequisite Software

A major prerequisite is to have a machine already running Helios. An easy way to
do this is by using a https://github.com/oxidecomputer/helios-engvm[Helios VM].
ISOs are also available for download https://pkg.oxide.computer/install[here].

Any additional prerequisite software may be installed with the following script:

[source,text]
----
$ ./tools/install_prerequisites.sh
----

=== Make me a Gimlet!

The sled agent expects to manage a real Gimlet. However, until those are built,
developers generally make do with something else, usually a commodity machine.
To make your machine "look" like a Gimlet, the
`./tools/create_virtual_hardware.sh` script can be used. This creates a few
file-based ZFS vdevs and ZFS zpools on top of those, and a couple of VNICs. The
vdevs model the actual U.2s that will be in a Gimlet, and the VNICs model the
two Chelsio NIC ports.

You can clean up these resources with `./tools/destroy_virtual_hardware.sh`. Be
aware that this is a best effort script. If the sled agent or other Omicron
zones are still running, the zpools cannot be deleted. The script will warn you
about the things it cannot remove, so you can do so yourself. Also note that all
the networking bits are temporary, so a reboot should always clear them.

Both scripts must be run as root, e.g, `pfexec ./tools/create_virtual_hardware.sh`.

== Deploying Omicron

The control plane repository contains a packaging tool which bundles binaries
and SMF manifests. After building the expected binaries, they can be packaged
in a format which lets them be transferred to a Helios machine.

This tool acts on a `package-manifest.toml` file which describes the packages to be
bundled in the build.

Configuration files are used to select IP addresses, and to manage Zpools
utilized by the Sled Agent. These configuration files are located within
`smf/`, and likely need to be modified to use addresses and zpools which match
your hardware. Much of this configuration will be automated in the future
(e.g., IP addresses will be inferred and posted to a DNS system, Zpools will
automatically be detected on discovered disks), but for now it remains
hard-coded.

[source,text]
----
$ cargo run --release --bin omicron-package -- package
----

NOTE: Running in `release` mode isn't strictly required, but improves
the performance of the packaging tools significantly.

The aforementioned package command fills a target directory of choice
(by default, `out/` within the omicron repository) with tarballs ready
to be unpacked as services.

To install the services on a target machine, the following command
may be executed:

[source,text]
----
$ pfexec cargo run --release --bin omicron-package -- install
----

This service installs a bootstrap service, which itself loads other
requested services. The bootstrap service is currently the only
service which is "persistent" across reboots - although it will
initialize other service as part of its setup sequence anyway.

[source,text]
----
# List all services:
$ svcs
# View zones managed by Omicron (prefixed with "oxz_"):
$ zoneadm list -cv
# View logs for a service:
$ pfexec tail -f $(pfexec svcs -z oxz_nexus -L nexus)
----

To uninstall all Omicron services from a machine, the following may be
executed:

[source,text]
----
$ pfexec cargo run --release --bin omicron-package -- uninstall
----

=== Test Environment

When we deploy, we're effectively creating a number of different zones
for all the components that make up Omicron (Nexus, Clickhouse, Crucible, etc).
Since all these services run in different zones they cannot communicate with
each other (and Sled Agent in the global zone) via `localhost`. In practice,
we'll assign addresses as per RFD 63 as well as incorporating DNS based
service discovery.

For the purposes of local development today, we specify some hardcoded IPv6
unique local addresses in the subnet of the first Sled Agent: `fd00:1122:3344:1::/64`:

[options="header"]
|===================================================================================================
| Service                    | Endpoint
| Sled Agent: Bootstrap      | Derived from MAC address of physical data link.
| Sled Agent: Dropshot API   | `[fd00:1122:3344:0101::1]:12345`
| Cockroach DB               | `[fd00:1122:3344:0101::2]:32221`
| Nexus: External API        | `[fd00:1122:3344:0101::3]:12220`
| Nexus: Internal API        | `[fd00:1122:3344:0101::3]:12221`
| Oximeter                   | `[fd00:1122:3344:0101::4]:12223`
| Clickhouse                 | `[fd00:1122:3344:0101::5]:8123`
| Crucible Downstairs 1      | `[fd00:1122:3344:0101::6]:32345`
| Crucible Downstairs 2      | `[fd00:1122:3344:0101::7]:32345`
| Crucible Downstairs 3      | `[fd00:1122:3344:0101::8]:32345`
| Internal DNS Service       | `[fd00:1122:3344:0001::1]:5353`
|===================================================================================================

Note that Sled Agent runs in the global zone and is the one responsible for bringing up all the other
other services and allocating them with vNICs and IPv6 addresses.

=== How to provision an instance using the CLI

Here are the current steps to provision an instance using the https://github.com/oxidecomputer/cli[oxide]
command line interface.

1. Create an organization and project that the resources will live under:

    oxide org create myorg
    oxide project create -o myorg myproj

2. Define a global image that will be used as initial disk contents. This can be the alpine.iso image that ships with propolis, or an ISO / raw disk image / etc hosted at a URL:

    oxide api /images --method POST --input - <<EOF
    {
      "name": "alpine",
      "description": "boot from propolis zone blob!",
      "block_size": 512,
      "distribution": {
        "name": "alpine",
        "version": "propolis-blob"
      },
      "source": {
        "type": "you_can_boot_anything_as_long_as_its_alpine"
      }
    }
    EOF

    oxide api /images --method POST --input - <<EOF
    {
      "name": "crucible-tester-sparse",
      "description": "boot from a url!",
      "block_size": 512,
      "distribution": {
        "name": "debian",
        "version": "9"
      },
      "source": {
        "type": "url",
        "url": "http://[fd00:1122:3344:101::15]/crucible-tester-sparse.img"
      }
    }
    EOF

3. Create a disk from that global image (note that disk size must be greater than or equal to image size!). The example below creates a disk using the image made from the alpine ISO that ships with propolis, and sets the size to be the exact same as the image size:

    oxide api /organizations/myorg/projects/myproj/disks/ --method POST --input - <<EOF
    {
      "name": "alpine",
      "description": "alpine.iso blob",
      "block_size": 512,
      "size": $(oxide api /images/alpine | jq -r .size),
      "disk_source": {
          "type": "global_image",
          "image_id": "$(oxide api /images/alpine | jq -r .id)"
      }
    }
    EOF

4. Create an instance, attaching the disk created above:

    oxide api /organizations/myorg/projects/myproj/instances --method POST --input - <<EOF
    {
      "name": "myinst",
      "description": "my inst",
      "hostname": "myinst",
      "memory": 1073741824,
      "ncpus": 2,
      "disks": [
        {
          "type": "attach",
          "name": "alpine"
        }
      ]
    }
    EOF

5. Optionally, attach to the propolis server serial console:

  a. find the zone launched for the instance: `zoneadm list -c | grep oxz_propolis-server`
  b. get the instance uuid from the zone name. if the zone's name is `oxz_propolis-server_3b03ad43-4e9b-4f3a-866c-238d9ec4ac45`, then the uuid is `3b03ad43-4e9b-4f3a-866c-238d9ec4ac45`
  c. find the propolis server listen address: `pfexec zlogin oxz_propolis-server_3b03ad43-4e9b-4f3a-866c-238d9ec4ac45 svccfg -s svc:/system/illumos/propolis-server:vm-3b03ad43-4e9b-4f3a-866c-238d9ec4ac45 listprop config/server_addr`
  d. build and launch propolis-cli, filling in values for IP and PORT based on the listen address: `./target/release/propolis-cli -s <IP> -p <PORT> serial`

