:showtitle:
:toc: left
:icons: font

= Running Omicron (Non-Simulated)

Omicron is the control plane for an Oxide rack. It expects to execute
on Helios systems, and Sleds use Helios-specific interfaces to manage
resources.

If you're interested in running the control plane on other platforms, including
Linux and Mac, refer to the guide on xref:how-to-run-simulated.adoc[running
simulated Omicron].

== Installing Prerequisite Software

A major prerequisite is to have a machine already running Helios. An easy way to
do this is by using a https://github.com/oxidecomputer/helios-engvm[Helios VM].
ISOs are also available for download https://pkg.oxide.computer/install[here].

The steps below will install several executables that will need to be in your
`PATH`.  You can set that up first using:

[source,text]
----
$ source env.sh
----

(You'll want to do this in the future in every shell where you work in this workspace.)

Then install prerequisite software with the following script:

[source,text]
----
$ pfexec ./tools/install_prerequisites.sh
----

You need to do this step once per workspace and potentially again each time you fetch new changes.  If the script reports any PATH problems, you'll need to correct those before proceeding.

This script expects that you are both attempting to compile code and execute
it on the same machine. If you'd like to have a different machine for a "builder"
and a "runner", you can use the two more fine-grained scripts:

[source,text]
----
# To be invoked on the machine building Omicron
$ ./tools/install_builder_prerequisites.sh
# To be invoked on the machine running Omicron
$ ./tools/install_runner_prerequisites.sh
----

Again, if these scripts report any PATH problems, you'll need to correct those before proceeding.

=== Make (or unmake) me a Gimlet!

The sled agent expects to manage a real Gimlet. However, until those are built,
developers generally make do with something else, usually a commodity machine.
To make your machine "look" like a Gimlet, the
`pfexec ./tools/create_virtual_hardware.sh` script can be used. This creates a few
file-based ZFS vdevs and ZFS zpools on top of those, and a couple of VNICs. The
vdevs model the actual U.2s that will be in a Gimlet, and the VNICs model the
two Chelsio NIC ports.

To access the outside world or other gimlets, we also set up a virtual sidecar
device for those network paths.

Optionally, the `PHYSICAL_LINK` environment variable can be set to override the
default logic used to automatically determine the physical network connection
used for external communication. For example:

----
$ PHYSICAL_LINK=ixgbe0 pfexec ./tools/create_virtual_hardware.sh
----

You can clean up these resources with `pfexec ./tools/destroy_virtual_hardware.sh`.
This script requires Omicron be uninstalled, e.g., with `pfexec
./target/release/omicron-package uninstall`, and a warning will be printed if
that is not the case. The script will then remove the file-based vdevs and the
VNICs created by `create_virtual_hardware.sh`.

=== Make me a certificate!

Nexus's external interface will typically be served using public-facing x.509
certificate. While we are still configuring the mechanism to integrate this real
certificate into the package system, `./tools/create_self_signed_cert.sh` can be
used to generate an equivalent self-signed certificate.

== Deploying Omicron

The control plane repository contains a packaging tool which bundles binaries
and SMF manifests. After building the expected binaries, they can be packaged
in a format which lets them be transferred to a Helios machine.

This tool acts on a `package-manifest.toml` file which describes the packages to be
bundled in the build.

Configuration files are used to select IP addresses, and to manage Zpools
utilized by the Sled Agent. These configuration files are located within
`smf/`, and likely need to be modified to use addresses and zpools which match
your hardware. Much of this configuration will be automated in the future
(e.g., IP addresses will be inferred and posted to a DNS system, Zpools will
automatically be detected on discovered disks), but for now it remains
hard-coded.

Packages also have a notion of "build targets", which are used to select
between different variants of certain components. A build target is composed
of an image type, a machine type, and a switch type:

[source,console]
----
$ cargo run --release --bin omicron-package -- target create -h
    Finished release [optimized] target(s) in 0.70s
     Running `target/release/omicron-package target create -h`
Error: Creates a new build target, and sets it as "active"

Usage: omicron-package target create [OPTIONS]

Options:
  -i, --image <IMAGE>      [default: standard] [possible values: standard, trampoline]
  -m, --machine <MACHINE>  [possible values: gimlet, gimlet-standalone, non-gimlet]
  -s, --switch <SWITCH>    [possible values: asic, stub, softnpu]
  -h, --help               Print help (see more with '--help')

----

To setup a build target for a non-Gimlet machine with external networking, you
would run:

[source,console]
----
$ cargo run --release --bin omicron-package -- -t default target create -i standard -m non-gimlet -s softnpu
    Finished release [optimized] target(s) in 0.66s
     Running `target/release/omicron-package -t default target create -i standard -m non-gimlet -s softnpu`
Created new build target 'default' and set it as active
----

NOTE: The `target create` command will set the new target as active and thus let you
omit the `-t` flag in subsequent commands.

=== Building

To actually kick off the build and package everything, you can run:

[source,console]
----
cargo run --release --bin omicron-package -- package
----

This will take all the packages defined in the manifest and further selected
by the active build target and either build them (if local) or download them
before packaging them into tarballs. The final artifacts will be placed in
a target directory of your choice (by default, `out/`) ready to be unpacked
as services.

NOTE: Running in `release` mode isn't strictly required, but improves
the performance of the packaging tools significantly.

NOTE: Instead of `package` you can also use the `check` subcommand to just
essentially run `cargo check` without building or creating packages.

=== Installing

To install the services on a target machine, the following command
may be executed:

[source,console]
----
$ cargo build --release --bin omicron-package
$ pfexec ./target/release/omicron-package install
----

[NOTE]
====
**Do not use `pfexec cargo run` directly**; it will cause files in `~/.cargo` and `target/` to be owned by root, which will cause problems down the road.

If you've done this already, and you wish to recover, run from the root of this repository `pfexec chown -R $USER:$(id -ng $USER) target ${CARGO_HOME:-~/.cargo}`.
====

This command installs a bootstrap service called
`svc:/oxide/sled-agent:default`, which itself loads other requested
services. The bootstrap service is currently the only service which is
"persistent" across reboots - although it will initialize other services as part
of its setup sequence anyway.

The first time the bootstrap service runs, it will take a while to initialize
the Omicron zones:

[source,console]
----
# List all services:
$ svcs

# View logs for sled-agent:
$ tail -F $(svcs -L sled-agent)
----

Once the zones are initialized, they'll show up in `zoneadm`:

[source,console]
----
# View zones managed by Omicron (prefixed with "oxz_"):
$ zoneadm list -cnv

# View logs for a service:
$ pfexec tail -f $(pfexec svcs -z oxz_nexus -L nexus)
----

=== Uninstalling

To uninstall all Omicron services from a machine, the following may be
executed:

[source,console]
----
$ cargo build --release --bin omicron-package
$ pfexec ./target/release/omicron-package uninstall
----

Once all the omicron services are uninstalled, you can also remove the
previously created virtual hardware as mentioned above:

[source,console]
----
$ pfexec ./tools/destroy_virtual_hardware.sh
----

==== Switch Zone

In a real rack, two of the Gimlets (referred to as Scrimlets) will be connected
directly to the switch (Sidecar). Those sleds will thus be configured with a switch
zone (`oxz_switch`) used to manage the switch. The `sled_mode` option in Sled Agent's
config will indicate whether the sled its running on is potentially a Scrimlet or Gimlet.

The relevant config will be in `smf/sled-agent/$MACHINE/config.toml`, where `$MACHINE`
is the machine type (e.g. `gimlet`, `gimlet-standalone`, `non-gimlet`) as specified
in the build target.

[source,text]
----
# Identifies whether sled agent treats itself as a scrimlet or a gimlet.
#
# If this is set to "scrimlet", the sled agent treats itself as a scrimlet.
# If this is set to "gimlet", the sled agent treats itself as a gimlet.
# If this is set to "auto":
# - On illumos, the sled automatically detects whether or not it is a scrimlet.
# - On all other platforms, the sled assumes it is a gimlet.
sled_mode = "scrimlet"
----

Once Sled Agent has been configured to run as a Scrimlet (whether explicitly or
implicitly), it will attempt to create and start the switch zone. This will
depend on the switch type that was specified in the build target:

1. `asic` implies we're running on a real Gimlet and are directly attached to the
Tofino ASIC.
2. `stub` provides a stubbed out switch implementation that doesn't
require any hardware.
3. `softnpu` provides a simulated switch implementation that
runs the same P4 program as the ASIC, but in software.

For the purposes of local development, the `softnpu` switch provides is used.
Unfortunately, Omicron does not currently automatically configure the switch
with respect to external networking, so you'll need to manually do so.

After installing omicron with `omicron-package install`, you can run the
`softnpu-init.sh` script to configure SoftNPU. By default, it'll attempt to
automatically detect your local network's gateway IP and MAC but those can
be overridden by setting the `GATEWAY_IP` and `GATEWAY_MAC` environment
variables, respectively.

[source,console]
----
$ ./tools/scrimlet/softnpu-init.sh
----

=== Test Environment

When we deploy, we're effectively creating a number of different zones
for all the components that make up Omicron (Nexus, Clickhouse, Crucible, etc).
Since all these services run in different zones they cannot communicate with
each other (and Sled Agent in the global zone) via `localhost`. In practice,
we'll assign addresses as per RFD 63 as well as incorporating DNS based
service discovery.

For the purposes of local development today, we specify some hardcoded IPv6
unique local addresses in the subnet of the first Sled Agent: `fd00:1122:3344:1::/64`.

If you'd like to modify these values to suit your local network, you can modify
them within the https://github.com/oxidecomputer/omicron/tree/main/smf[`smf/` subdirectory].
Notably, Nexus is being served from IPv4 address, which may be configured to be
external. By default, it uses a private IPv4 address but may be configured to use
a public-facing IP address.

NOTE: Internal services that require external connectivity (e.g. Nexus, Boundary NTP,
External DNS) do so via OPTE. When using SoftNPU we'll need to configure Proxy ARP for
the services IP Pool.

[source,console]
----
# dladm won't return leading zeroes but `scadm` expects them
$ SOFTNPU_MAC=$(dladm show-vnic sc0_1 -p -o macaddress | gsed 's/\b\(\w\)\b/0\1/g')
$ pfexec /opt/oxide/softnpu/stuff/scadm \
  --server /opt/oxide/softnpu/stuff/server \
  --client /opt/oxide/softnpu/stuff/client \
  standalone \
  add-proxy-arp \
  $SERVICE_IP_POOL_START \
  $SERVICE_IP_POOL_END \
  $SOFTNPU_MAC
----

[options="header"]
|===================================================================================================
| Service                    | Endpoint
| Sled Agent: Bootstrap      | Derived from MAC address of physical data link.
| Sled Agent: Dropshot API   | `[fd00:1122:3344:0101::1]:12345`
| Switch Zone                | `[fd00:1122:3344:0101::2]`
| Cockroach DB               | `[fd00:1122:3344:0101::3]:32221`
| Nexus: Internal API        | `[fd00:1122:3344:0101::4]:12221`
| Oximeter                   | `[fd00:1122:3344:0101::5]:12223`
| Clickhouse                 | `[fd00:1122:3344:0101::6]:8123`
| Crucible Downstairs 1      | `[fd00:1122:3344:0101::7]:32345`
| Crucible Downstairs 2      | `[fd00:1122:3344:0101::8]:32345`
| Crucible Downstairs 3      | `[fd00:1122:3344:0101::9]:32345`
| Internal DNS Service       | `[fd00:1122:3344:0001::1]:5353`
| External DNS               | `192.168.1.20:53`
| Nexus: External API        | `192.168.1.21:80`
|===================================================================================================

Note that Sled Agent runs in the global zone and is the one responsible for bringing up all the other
other services and allocating them with vNICs and IPv6 addresses.

=== How to provision an instance using the CLI

Here are the current steps to provision an instance using the https://github.com/oxidecomputer/cli[oxide]
command line interface.  Note that the `jq` command is required. In addition, the examples build on each other, so a previous name (or org, or project) are used in later steps.

1. Create a project that the resources will live under:

    oxide project create myproj

2. Create an IP Pool, for providing external connectivity to the instance later.
We need to create an IP Pool itself, and a range of IP addresses in that pool.
**Important:** The addresses used here are appropriate for the Oxide lab
environment, but not for an arbitrary environment. The actual IP range must
currently be something that matches the physical network that the host is
running in, at least if you'd like to be able to SSH into the guest. This is
most often a private address range, like `10.0.0.0/8` or `192.168.0.0/16`, but
the exact addresses that are available depends on the environment.
+
[source,console]
----
$ oxide api /v1/system/ip-pools/default/ranges/add --method POST --input - <<EOF
{
  "first": "172.20.15.227",
  "last": "172.20.15.239"
}
EOF
----
+
Additionally, if you're using SoftNPU and your chosen IP range is on the same
L2 network as the router or other non-oxide hosts, you'll need to configure
Proxy ARP:
+
[source,console]
----
# dladm won't return leading zeroes but `scadm` expects them
$ SOFTNPU_MAC=$(dladm show-vnic sc0_1 -p -o macaddress | gsed 's/\b\(\w\)\b/0\1/g')
$ pfexec /opt/oxide/softnpu/stuff/scadm \
  --server /opt/oxide/softnpu/stuff/server \
  --client /opt/oxide/softnpu/stuff/client \
  standalone \
  add-proxy-arp \
  $IP_POOL_START \
  $IP_POOL_END \
  $SOFTNPU_MAC
----

3. Define a project image that will be used as initial disk contents.

 a. This can be the alpine.iso image that ships with propolis:

    oxide api /v1/images?project=<proj> --method POST --input - <<EOF
    {
      "name": "alpine",
      "description": "boot from propolis zone blob!",
      "block_size": 512,
      "distribution": {
        "name": "alpine",
        "version": "propolis-blob"
      },
      "source": {
        "type": "you_can_boot_anything_as_long_as_its_alpine"
      }
    }
    EOF

 b. Or an ISO / raw disk image / etc hosted at a URL:

    oxide api /v1/images --method POST --input - <<EOF
    {
      "name": "crucible-tester-sparse",
      "description": "boot from a url!",
      "block_size": 512,
      "distribution": {
        "name": "debian",
        "version": "9"
      },
      "source": {
        "type": "url",
        "url": "http://[fd00:1122:3344:101::15]/crucible-tester-sparse.img"
      }
    }
    EOF

4. Create a disk from that image (note that disk size must be greater than or equal to image size and a 1GiB multiple!). The example below creates a disk using the image made from the alpine ISO that ships with propolis, and sets the size to the next 1GiB multiple of the original alpine source:

    oxide api /v1/disks?project=myproj --method POST --input - <<EOF
    {
      "name": "alpine",
      "description": "alpine.iso blob",
      "block_size": 512,
      "size": 1073741824,
      "disk_source": {
          "type": "image",
          "image_id": "$(oxide api /v1/images/alpine | jq -r .id)"
      }
    }
    EOF

5. Create an instance, attaching the alpine disk created above:

    oxide api /v1/instances?project=myproj --method POST --input - <<EOF
    {
      "name": "myinst",
      "description": "my inst",
      "hostname": "myinst",
      "memory": 1073741824,
      "ncpus": 2,
      "disks": [
        {
          "type": "attach",
          "name": "alpine"
        }
      ],
      "external_ips": [{"type": "ephemeral"}]
    }
    EOF

6. Optionally, attach to the proxied propolis server serial console (this requires https://github.com/oxidecomputer/cli/commit/adab246142270778db7208126fb03724f5d35858[this commit] or newer of the CLI.)

    oxide instance serial --interactive -p myproj -o myorg myinst

== Building host images

Host images for both the standard omicron install and the trampoline/recovery
install are built as a part of CI. To build them locally, first run the CI
script:

[source,console]
----
$ ./.github/buildomat/jobs/package.sh
----

This will create a `/work` directory with a few tarballs in it. Building a host
image requires a checkout of
https://github.com/oxidecomputer/helios-engvm[helios]; the instructions below
use `$HELIOS_PATH` for the path to this repository. To match CI builds, you
should check out the commit specified in `./tools/helios_version`. (The script
will check your current commit hash and will refuse to run if it doesn't match
unless you pass `-f`.)

To build a standard host image:

[source,console]
----
$ ./tools/build-host-image.sh -B $HELIOS_PATH /work/global-zone-packages.tar.gz
----

To build a recovery host image:

[source,console]
----
$ ./tools/build-host-image.sh -R $HELIOS_PATH /work/trampoline-global-zone-packages.tar.gz
----
