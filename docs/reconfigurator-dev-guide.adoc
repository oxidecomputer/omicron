:showtitle:
:numbered:
:toc: left

= Reconfigurator Developer Guide

This document covers practical tips for working on Reconfigurator.  For principles and design, see xref:reconfigurator.adoc[Reconfigurator Overview].

== Introduction

Reconfigurator is a control plane subsystem that's responsible for runtime changes to the control plane.  It's used to add, remove, and upgrade components.  It's divided into two big pieces:

* The **planner** generates **blueprints**, which are complete descriptions of how the system _should_ look (in terms of what components exist, at what versions, etc.)
* The **executor** takes a given blueprint and attempts to make reality match it.

Blueprints are stored in CockroachDB (the control plane database).  This makes them available to all Nexus instances.  It also ensures strong consistency in what the system's current blueprint is supposed to be.

Reconfigurator is designed to run autonomously as part of **Nexus** (see xref:control-plane-architecture.adoc[]).  But as much as possible, the pieces are factored into self-contained packages that don't know about most of Nexus.  As a concrete example:

* Autonomous blueprint execution in real systems is driven by the Nexus `blueprint_execution` background task.  But that task essentially just invokes `nexus_reconfigurator_execution::realize_blueprint`.
* Execution itself is encapsulated within this `nexus_reconfigurator_execution` package.

This has some big benefits:

* When working on blueprint execution, you usually only need to run `cargo check` and `cargo test` on the `nexus_reconfigurator_execution` package.  You don't need to build and link Nexus (which involves a lot more code and takes a lot more time).
* It's possible to build developer tools like `reconfigurator-exec-unsafe`, which directly uses the `nexus_reconfigurator_execution` package.  This gives developers finer control over blueprint execution and more direct visibility, while still using the exact same interfaces that the autonomous system in Nexus is using.

It does mean there are lots of layers, though.  Here's a conceptual map of components involved in blueprint execution:

```mermaid
graph TD
    Executor["Executor"]
    MgsUpdateDriver["MgsUpdateDriver<br />(updates SPs, RoTs, etc.)"]
    LiveSystem["Live System<br />(rack/racklette/a4x2/simulated))"]

    subgraph Nexus ["Nexus (real systems)"]
        NexusUsesExecutor["Executor<br/>(background task)"]
    end

    %% Dev tools - right side: Executor tools
    subgraph ExecutorTools ["Execution Tools (dev/test)"]
        ExecUnsafe(["reconfigurator-exec-unsafe<br/>(execute blueprints manually)"])
        SPUpdater(["reconfigurator-sp-updater<br/>(updates SPs, RoTs, etc.)"])
    end

    NexusUsesExecutor --> |blueprint from database| Executor
    ExecUnsafe -->|blueprint from file| Executor

    Executor -->|Modifies| LiveSystem
    Executor --> |input: from blueprint| MgsUpdateDriver
    MgsUpdateDriver --> |Modifies| LiveSystem
    SPUpdater --> |input: REPL| MgsUpdateDriver

    %% Styling
    style Nexus fill:#c8e6c9,stroke:#388e3c
    style ExecutorTools fill:#f3e5f5,stroke:#8e24aa
    style Executor fill:#ffe0b2,stroke:#fb8c00
    style MgsUpdateDriver fill:#ffe0b2,stroke:#fb8c00
```

// XXX-dap fix diagrams not working well in dark mode

== Key Rust packages

Below are some of the most important Rust packages to know about.  This is not a complete list.

.Key Rust packages used in Reconfigurator
[cols="1,1,3",options="header"]
|===
h|Area
|Omicron repo path (Rust package)
|Description

.2+|Not Reconfigurator-specific

|`nexus/types` (`nexus_types`)
|Very widely-used package containing types common to many parts of Nexus (shared with lots of components that are used within Nexus, but aren't generally aware of the rest of Nexus).  (Not Reconfigurator-specific.)

|`nexus/db-model`/`nexus/db-queries` (`nexus_db_model`, `nexus_db_queries`)
|Everything related to the control plane database: Rust types representing the database schema itself, types that model the various tables in the database, and implementations of database queries that fetch/insert/update/delete data in the database.  (Not Reconfigurator-specific.)

.4+|Reconfigurator (and Reconfigurator-adjacent)

|`nexus/inventory` (`nexus_inventory`)
|Inventory subsystem.  Collects information from the whole system about its current state, stores it in the database, and makes it available to the rest of Nexus.  Inventory collection is driven periodically and on-demand by a Nexus background task that just calls into this package.

|`nexus/reconfigurator/planning` (`nexus_reconfigurator_planning`)
|Implementation of the planner.  Currently, this is driven only by explicit calls to the Nexus internal API, which in turn come from a person running `omdb`.  In the medium term, this will be driven periodically and on-demand by a Nexus background task.

|`nexus/reconfigurator/execution` (`nexus_reconfigurator_execution`)
|Implementation of blueprint execution.  Blueprint execution is driven periodically and on-demand by a Nexus background task that just calls into this package.

|`nexus/mgs-updates` (`nexus_mgs_updates`)
a|Implementation of software update for components that are updated through Management Gateway Service (MGS) and the service processor (SP).  This includes the service processor Hubris image, the root of trust Hubris image, the root of trust bootloader, and phase 1 of the host operating system (the part that's stored in flash).

This is used as part of execution.
|===

== Developer tools

.Key developer tools for working on Reconfigurator
[cols="1,1,1,3",options="header"]
|===
h|Area
|Tool
|Omicron repo path
|Description

.4+|Reconfigurator-specific
|`reconfigurator-cli`
|`dev-tools/reconfigurator-cli`
|Directly edit blueprints or run the planner in-memory.  Can import state from real systems and export blueprints back to real systems.  Essential tool for observing and testing planner behavior and for generating blueprints that a real system might not otherwise do.  This in turn is useful for development and for operational emergencies.

|`reconfigurator-exec-unsafe`
|`dev-tools/reconfigurator-exec-unsafe`
|Directly execute blueprints against a live system (outside the context of Nexus).  The main use of this tool is to be able to precisely control blueprint execution (usually for testing) and to be able to execute blueprints whose JSON representation does not match the database representation (common while features are under development, but never expected in a real system).

|`reconfigurator-sp-updater`
|`dev-tools/reconfigurator-sp-updater`
|Directly runs Reconfigurator-style updates of MGS/SP-managed software.  This is used for development and testing of `nexus_mgs_updates` without having to create blueprints or go through real blueprint execution.

|`repo-depot-standalone`
|`dev-tools/repo-depot-standalone`
a|Standalone command line tool for serving the Repo Depot API (which serves TUF repo artifacts over HTTP) from any TUF repository in your local filesystem.
+
This is especially useful with `reconfigurator-sp-updater`.

.2+|Non-Reconfigurator-specific (general tools)
|`omdb`
|`dev-tools/omdb`
a|`omdb` is a general tool for inspecting and controlling various Omicron components.

* You can control blueprint planning and execution with `omdb nexus blueprints`.
* You can monitor blueprint execution with `omdb nexus background-tasks show blueprint_executor`.
* You can view database state with `omdb db` (e.g., `omdb db inventory collections show latest`).

|`cargo xtask omicron-dev run-all`
|`dev-tools/omicron-dev`
|Stands up a whole control plane using simulated sled agents.  This is by far the quickest and simplest way to test quite a lot of the system, but of course has limitations on what it's able to simulate.

|===

Here's a conceptual map of components involved in planning and execution and the tools you can use to work on them directly:

```mermaid
graph TD
    Planner["Planner / Blueprint Editor"]
    subgraph Nexus ["Nexus (real systems)"]
        NexusUsesPlanner["Planner<br/>(background task)<br/>(eventually)"]
        NexusUsesExecutor["Executor<br/>(background task)"]
    end

    NexusUsesPlanner -->|blueprint: <br />from database| Planner

    subgraph PlannerTools ["Planner Tools (dev/test/support)"]
        CLI(["reconfigurator-cli<br />(dev/test/support tool)"])
    end
    CLI -->|"blueprint: synthetic (REPL) or loaded from a real system"| Planner

    style Nexus fill:#c8e6c9,stroke:#388e3c
    style Planner fill:#ffe0b2,stroke:#fb8c00
    style PlannerTools fill:#f3e5f5,stroke:#8e24aa

    Executor["Executor"]
    MgsUpdateDriver["MgsUpdateDriver<br />(updates SPs, RoTs, etc.)"]
    LiveSystem["Live System<br />(rack/racklette/a4x2/simulated))"]

    %% Dev tools - right side: Executor tools
    subgraph ExecutorTools ["Execution Tools (dev/test)"]
        ExecUnsafe(["reconfigurator-exec-unsafe<br/>(execute blueprints manually)"])
        SPUpdater(["reconfigurator-sp-updater<br/>(updates SPs, RoTs, etc.)"])
    end

    NexusUsesExecutor --> |blueprint: from database| Executor
    ExecUnsafe -->|blueprint: from file| Executor

    Executor -->|Modifies| LiveSystem
    Executor --> |input: from blueprint| MgsUpdateDriver
    MgsUpdateDriver --> |Modifies| LiveSystem
    SPUpdater --> |input: REPL| MgsUpdateDriver


    %% Styling
    style Nexus fill:#c8e6c9,stroke:#388e3c
    style ExecutorTools fill:#f3e5f5,stroke:#8e24aa
    style Executor fill:#ffe0b2,stroke:#fb8c00
    style MgsUpdateDriver fill:#ffe0b2,stroke:#fb8c00

```

== Nexus background tasks

Background operations in the control plane are driven by Nexus **background tasks**.  See xref:../nexus/src/app/background/mod.rs[] for important background on the design of background tasks.  Most importantly, the system has been designed to streamline writing background activities that:

* correctly handle crashing in the middle of execution
* correctly handle being executed concurrently (in other Nexus instances)
* make their status observable
* can be activated on-demand by a developer or support technician

Again, there's a lot more about this in the comment in the file linked above.

**In general, the Rust module that implements the background task does almost nothing except call into an implementation that's in some other Rust package.**  Generally, this approach:

* Makes it easier to write comprehensive tests for the background task.  That's because the background task abstraction itself is intentionally very opaque.  It just has one `activate()` function.  So to test it exhaustively, it's helpful to put the bulk of the implementation into something with a richer interface for control and observability.
* Makes it faster to iterate on the implementation because you need only run `cargo check`, `cargo nextest`, etc. on your implementation package, which usually won't require building and linking the rest of Nexus.  By contrast, the background tasks themselves are part of Nexus so rebuilding them takes more time.

Each background task has a fixed name (e.g., `blueprint_executor`).  You can use `omdb nexus background-tasks` to list, activate, observe the status of background tasks.

Here are the most important background tasks related to Reconfigurator:

.Key Reconfigurator-related background tasks
[cols="1h,4",options="header"]
|===
|Task name
|Description

|`inventory_collection`
|Fetches information about the current state of all hardware and software in the system (the whole rack)

|`blueprint_executor`
|Executes the most recently loaded blueprint

|`blueprint_loader`
|Loads the latest target blueprint from the database

|`blueprint_rendezvous`
|Updates rendezvous tables based on the most recent target blueprint

|`dns_config_internal`, `dns_servers_internal`, `dns_propagation_internal`,
`dns_config_external`, `dns_servers_external`, `dns_propagation_external`
|Drives the propagation of internal and external DNS.  Configuration changes start in Nexus and get written to the database.  Then these background tasks load the configuration (`dns_config_*`), load the list of servers to propagate it to (`dns_servers_*`), and propagate the config to the servers (`dns_propagation_*`).

|`tuf_artifact_replication`
|Distributes all artifact files in all user-uploaded TUF repositories to all sleds

|===

Many other tasks work with Reconfigurator, too (e.g., region replacement and region snapshot replacement).

Notably absent from this list is anything related to planning.  This has not been automated as a background task yet.

== Testing and developer workflow

There are a bunch of different environments that you can set up and use to test Omicron.

.Kinds of Omicron test environments
[cols="1,2,2a,2a,2a",options="header"]
|===
|Name
|Summary
|Pros
|Good for
|Limitations

|xref:how-to-run-simulated.adoc[`cargo xtask omicron-dev run-all`]
|Command-line tool that stands up real instances of much of the control plane locally (in-process and child processes): Nexus, CockroachDB, Clickhouse, Management Gateway Service, Oximeter, Crucible Pantry.  Limitations result from using simulated sled agent, simulated service processors, and loopback networking.
* Easy (one command), quick (starts in ~10s)
* Fast to iterate (rebuilds in a minute or two, depending on what component you're changing)
* Exactly matches the environment provided to Nexus integration tests (so it can be useful for developing and debugging these tests).
|
* Nexus internal/external API changes
* Most of development for anything that can be simulated (e.g., inventory, most parts of execution)
* `omdb`-only changes
|
* Simulated sled agent has many limitations: cannot run VMs, does not simulate the actual control plane components that it pretends to run, no simulation of Crucible storage, etc.
* Simulated SPs have limited fidelity to the real thing (e.g., resetting SP will not simulate reset of the sled, even though a real one would)
* No Wicket, no full RSS path
* No meaningful simulation of networking (so can't be used to test behavior of underlay connectivity, external connectivity, configuring Dendrite, etc.)

|https://github.com/oxidecomputer/testbed/tree/main/a4x2[`a4x2`]
|Uses VMs, fancy local networking config, and a software-based switch (https://github.com/oxidecomputer/softnpu[softnpu]) to create a multi-sled environment that looks much more realistic to the control plane than `omicron-dev run-all`.
|
* Much higher fidelity to real systems than `omicron-dev run-all`:
** most components' environments look largely like a real system (e.g., run in a zone, using the SMF start methods)
** softnpu implements the same (runtime-configurable) networking behavior that real switches do
** real sled agent runs real instances of all components except simulated networking (which is full-fidelity) and simulated service processors
|
* More time required up front to get started (may need beefier dev machine)
* Somewhat bumpy developer experience (see README)
* Longer iteration time (rebuild and redeploy takes ~30-60 minutes)
* Limitations in fidelity:
** Cannot run instances (sleds are running in VMs and we don't support nested virt)
** Service processors are simulated (just like `omicron-dev run-all`)

|xref:how-to-run.adoc[`Running non-simulated Omicron on a single system`]
|Runs real Sled Agent and all other components directly on your dev system the same way they'd run on a real system
|
* Moderate iteration time (rebuild and redeploy could take minutes, depending on what you're changing)
* Could support running VMs
| ?
|
* "Takes over" your dev system -- does not clearly delineate what global state it's responsible for and have a way to clean it all up
* Somewhat brittle (e.g., after reboot, SMF service for sled agent may start but not find the files it needs)
* Limitations in fidelity:
** Only one sled
** No service processors
** Networking simulation is incomplete (connectivity depends on how your dev system is set up)

|Racklette
|Real Oxide hardware (sleds and switches), essentially indistinguishable from a real Oxide rack
|Everything.  Worthwhile for:
* any testing involving real "customer" VMs
* final smoke testing for work developed with simulated components
|
* Very limited, shared resource
|===

https://github.com/oxidecomputer/omicron/pull/7424[Work is ongoing] to add `cargo xtask` commands for launching an a4x2 environment.  This would significantly streamline the process of using a4x2 and also make it possible to use a4x2 in CI.

A common development workflow is:

* "inner loop" as you work on code: run `cargo check`
* some combination of:
** use `cargo xtask omicron-dev run-all` and various developer tools to test it out
** add unit tests run with `cargo nextest run`
* once things are working, test end-to-end on a4x2 (if that's faithful enough) or a racklette

== Automated testing

Broadly, we have several kinds of tests:

* Various levels of unit test and small-scale integration tests for most components, including the planner, execution, etc.  The integration tests use an environment identical to `cargo xtask omicron-dev run-all`.
* For testing the planner and blueprint builder: we have `reconfigurator-cli` _scripts_ that run a bunch of commands print the contents of blueprints and diffs between blueprints and verify that these look like we expect.
* Omicron CI runs xref:../end-to-end-tests["end-to-end"] tests in the "Running non-simulated Omicron on a single system" environment.
* We have a small number of xref:../live-tests["live tests"] that can be run on-demand in a4x2 or a racklette that exercise behavior that can't currently be tested in CI.

The https://github.com/oxidecomputer/omicron/pull/7424[ongoing work mentioned above] will make it possible to run the live tests in a4x2 in CI.

== Updates for SPs, RoTs, etc.

Updates for the following components get lumped together:

* service processor Hubris image
* root of trust Hubris image
* root of trust bootloader Hubris image
* host OS phase 1 image

That's because all of these are managed by the service processor (SP).  They all follow a similar flow.  The control plane talks to SPs through Management Gateway Service, so we often call these MGS-managed updates or just "MGS Updates" (or sometimes "SP-managed updates").

There are a few ways to update SPs and their associated components:

* via Wicket, which uses MGS to deploy an artifact from the TUF repo.  This is the way we update most systems in development and production today.  Since you're supplying the TUF repo, Wicket is doing the work to figure out which artifact is appropriate for the hardware being updated.
* via `faux-mgs`, which talks directly to the SP and deploys an image directly from a file you give it.  Since you're giving it the specific file to use, you do the work of figuring out what that should be (e.g., picking which artifact from a TUF repo is appropriate for the hardware you're updating).  Updating with `faux-mgs` is outside the scope of this document but there's some information and links below on how to do this.
* via `humility` or other low-level tools (outside the scope of this document)
* "Reconfigurator-driven": what this section is about.

"Reconfigurator-driven" means that we're using `nexus_mgs_updates` to perform the update.  That implementation is designed to support:

* updating to software images stored in a TUF repository
* resuming after crashing at any point
* executing concurrently (in different Nexus instances)

The easiest way to test Reconfigurator-driven updates is using `reconfigurator-sp-updater` (more on this below).  You can also use `reconfigurator-cli` to generate a blueprint that specifies an MGS-managed update and then use `reconfigurator-exec-unsafe` to execute it.  This is more cumbersome but tests the integration of `nexus_mgs_updates` into blueprint execution.  (That's pretty simple and tested at this point so this is probably not a very useful flow unless something is broken.)  Eventually, you'll be able to test these updates through normal, Nexus-driven blueprint execution.  This is blocked on database support for the parts of blueprints that specify MGS-managed updates.

Regardless of how you perform updates, it's useful to use `faux-mgs` to read the ground truth state from the SP about its configuration (what versions are in each slot and which slots are active).  More on this below.

=== Task: manually performing Reconfigurator-driven update of SPs and RoTs

. Decide what software you want to deploy.  This must be packaged in a TUF repository.
+
If you're just testing update and don't care what you're deploying, you can use one generated by the CI process from any commit on "main".
2. Figure out which artifact within the TUF repository you need to use for your hardware.
+
In all cases, you can either look at the metadata in the unpacked TUF repo (`jq < repo/targets/*.artifacts.json`) or just look at the filenames of the artifacts (`ls repo/targets`).
** For service processors: the image should reflect the type of board you're updating (`kind` should include `switch` or `gimlet` or `psc`)
3. Use `repo-depot-standalone` to serve the TUF repo depot API backed by the TUF repo you want to use.
4. Use `reconfigurator-sp-updater` to perform the update.

// XXX-dap working here
// XXX-dap create separate task sections for the different pieces here

=== Setting up `faux-mgs`

https://github.com/oxidecomputer/management-gateway-service/tree/main/faux-mgs[`faux-mgs`] is a command-line tool that talks directly to SPs (without using MGS).  For Omicron developers, it's the lowest level tool we usually need to directly inspect SP state and issue commands to the SP.

This tool is most useful for:

* directly inspecting the current SP state (while debugging or learning)
* manually performing SP-managed updates as part of understanding how they work

To use: first clone the above repo and build with `cargo build --bin=faux-mgs`.

For racklettes: copy this binary to the switch zone and run it from there.  Use `faux-mgs --interface gimlet14 ...` to use it against the SP for sled 14 (just as an example).  Use `dladm show-vlan` in the switch zone to see what other interfaces exist to talk to switches, PSCs, etc.

For a4x2: copy this binary to the switch zone and run it from there.  You'll need to find the IP and ports of the simulated SPs running in this zone.  TODO how do you do that?

For `omicron-dev run-all`, you can run this command from the same system where you're running `omicron-dev`.  Instead of `--interface`, you need to use the `--sp-sim-addr IPV6_ADDR:PORT` option to point `faux-mgs` at the simulated SP.  Unfortunately, the easiest way to find the address and port of the simulated SP is in the log file whose path is printed out by `omicron-dev run-all`.

---

However you get `faux-mgs` running, you can use it to inspect state and https://github.com/oxidecomputer/meta/blob/master/engineering/mupdate/manual-rot-sp-updates.adoc[perform updates by hand].  (If you follow those linked instructions, note that they use `pilot sp exec -e CMD SERIAL`.  This is a thin wrapper that finds the right interface for the host with serial `SERIAL` and then runs `faux-mgs --interface INTERFACE CMD`.  You can just do this transformation yourself.)

The most useful commands for inspecting state are:

* `faux-mgs ... state`: summarizes the SP and RoT information
* `faux-mgs ... update-status`: reports whether any SP-managed update is in progress
* `faux-mgs ... read-component-caboose`: reports one piece of metadata about the software in a particular firmware slot.  You need to specify the component (e.g., `sp` or `rot`), the slot (e.g., `0` or `1`), and the key (`VERS` for version, `SIGN` for a hash of the signing key, etc.)

Also useful are:

* `faux-mgs ... reset`: resets a componnet (SP, RoT, etc.)
* `faux-mgs ... update`: uploads a new software image for a particular component (SP, RoT, etc.) slot

=== Using `omdb` to read inventory

The system inventory includes all the information we need about SPs and what software they're running.  You can print this with:

```
$ omdb db inventory collections show latest sp
...

Sled SimGimlet00
    part number: i86pc
    power:    A2
    revision: 0
    MGS slot: Sled 0 (cubby 0)
    found at: 2025-05-22 21:49:54.267308 UTC from http://[::1]:63421
    cabooses:
        SLOT       BOARD        NAME         VERSION GIT_COMMIT
        SpSlot0    SimGimletSp  SimGimlet    0.0.2   ffffffff
        SpSlot1    SimGimletSp  SimGimlet    0.0.1   fefefefe
        RotSlotA   SimRot       SimGimletRot 0.0.4   eeeeeeee
        RotSlotB   SimRot       SimGimletRot 0.0.3   edededed
        Stage0     SimRotStage0 SimGimletRot 0.0.200 ddddddddd
        Stage0Next SimRotStage0 SimGimletRot 0.0.200 dadadadad
    RoT pages:
        SLOT         DATA_BASE64
        Cmpa         Z2ltbGV0LWNtcGEAAAAAAAAAAAAAAAAA...
        CfpaActive   Z2ltbGV0LWNmcGEtYWN0aXZlAAAAAAAA...
        CfpaInactive Z2ltbGV0LWNmcGEtaW5hY3RpdmUAAAAA...
        CfpaScratch  Z2ltbGV0LWNmcGEtc2NyYXRjaAAAAAAA...
    RoT: active slot: slot A
    RoT: persistent boot preference: slot A
    RoT: pending persistent boot preference: -
    RoT: transient boot preference: -
    RoT: slot A SHA3-256: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
    RoT: slot B SHA3-256: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb

...
```

// XXX-dap link to section showing how to trigger an inventory collection, which itself should link to a section on waiting for it to complete.
This is a handy summary, but it only gets updated when inventory is collected.  This is more cumbersome than `faux-mgs` when you only need to get one piece of information and need it to be up-to-date.


// XXX-dap task: generate a new blueprint using the planner
// XXX-dap task: export reconfigurator state
// XXX-dap task: generate a new blueprint using reconfigurator-cli
// XXX-dap task: import blueprint
// XXX-dap task: execute blueprint (via Nexus)
// XXX-dap task: monitor blueprint execution
// XXX-dap task: previewing what changes a blueprint will make
// XXX-dap task: trigger inventory collection
// XXX-dap task: wait for inventory collection to complete
// XXX-dap task: download a TUF repo from CI (and link this where we do it above)
// XXX-dap task: figure out which SP image you need
// XXX-dap task: figure out which RoT image you need from a TUF repo
// XXX-dap task: serve a local, unpacked TUF repo via repo-depot-API

// XXX-dap diagram showing:
// - planner creates blueprints and stores them into database
// - user can import blueprints with reconfigurator-cli
// - execution reads blueprints
