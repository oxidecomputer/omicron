:showtitle:
:numbered:
:toc: left

= Omicron repository

The Omicron git repo is a consolidation of several different components that work together but are deployed separately.  Developers new to Omicron can get tripped up by the size and complexity of the repo.  The README contains tips for working in Omicron, including how to iterate more quickly by only building parts of it.  This document explains more about why the repo is structured the way it is.  This largely comes from experience with previous systems that were similarly complicated but structured differently.

== Design goals

To help the development process, we seek:

* to have clear, up-to-date, and regularly tested instructions for setting up a development environment from scratch.
* to have clear instructions for basic activities like formatting code, running clippy, running tests, etc.  These should be consistent across components and across local development vs. CI.
* to prioritize debugging and fixing flaky tests so that developers can always expect the tests to pass.
* to ensure that a fresh clone and build of the repo should produce equivalent software to any other clone, including the CI environment.  If tests pass for one developer on the tip of "main", they should pass for other developers as well as CI.

To make it easy to run tests, the test suite automates setup and teardown of the service under test _and_ other services that it depends on (rather than relying on error-prone manual steps to set these up).

To make it easy to write comprehensive, reliable tests:

* Individual tests generally don't need to worry about interference from other tests.  That includes things like TCP ports in use or stale state in dependent services.
* With the above infrastructure for standing up transient instances of dependencies, it's possible to write end-to-end integration tests that exercise multiple components together with minimal boilerplate.

We regularly keep dependencies (including Rust itself) up to date so that we don't suddenly find we need to make big leaps in these.

Where we have stable interfaces, we seek to keep changelogs that describe for each breaking change how to know if you're affected and what you have to do to move past it.  These are written for maintainers of consumers, who might not be very familiar with a library's other recent changes or internal details.

---

Here are some of things we've done towards these goals:

* The dev environment setup process is https://github.com/oxidecomputer/omicron/blob/main/docs/how-to-run-simulated.adoc#installing-prerequisites[documented and mostly automated].  CI uses this script to go from a bare environment to one that builds and tests Omicron, so this automation is tested regularly with the rest of the repo.
* Omicron houses many related components in one repo.
** There's a clear place to put overall system documentation, including how to set up one's dev environment.
** There's a single set of steps for setting up your dev environment, formatting code, running clippy, and running tests, no matter which component(s) you're working on.  (You can still do any of this on individual components rather than the whole repo at once.)
** Various component test suites depend on other components in the repo so that they can spin up transient instances of them for testing.
** When we want to update a dependency used in many places, we can often just make one (git) change to update it everywhere.  (This doesn't solve the problem of _deploying_ all those changes, of course.)
* We've automated the process to spin up transient instances of most services: a standard `ControlPlaneTestContext` in the Nexus test suite spins up real instances of CockroachDB, Clickhouse, Nexus, and internal/external DNS; plus mock versions of Sled Agent and parts of Crucible.  All of this makes it very easy to write a new integration test that uses a great deal of the real stack.  You can also spin all this up and poke around interactively using the `omicron-dev run-all` command.
* Because spin-up/spin-down of these services is automated, every Nexus integration test gets its own copy of everything (including the database).  They don't have to worry about stomping on each other.  All server sockets use the pattern of binding to port 0 in the test suite (letting the OS pick the port), also so they don't stomp on each other or any other running software.
* To help stamp out build flakes, we keep around debugging information for failed tests (e.g., trace-level log files, database contents, etc.).  There are tools for spinning up a transient database instance atop an existing database storage directory so that you can easily inspect the contents after a failed test.
* We use dependabot and Renovate to keep dependencies updated, including Rust itself.
* We use rust-toolchain and Cargo.lock to ensure that developers are getting a consistent toolchain and packages as each other and CI.
* For many components used by Omicron, we create https://github.com/oxidecomputer/dropshot/blob/main/CHANGELOG.adoc#090-released-2023-01-20[changelogs] that describe for each breaking change how to know if you're affected and what you have to do to move past it.  (We don't yet do this for the tightly coupled components within Omicron.)

These are all related: using dependabot for keeping dependencies up to date is only tenable _because_ Rust is so well statically-checked (so the build usually breaks when a dependency makes a breaking change) and we have good test coverage so that it's generally fair to say that if CI passes, the change didn't break Omicron.  We have good test coverage in large part _because_ it's relatively easy to write reliable tests.  It's easy to write tests in part because they're in the same repo as the software, they operate on their own isolated copies of the stack, etc.

== Caveats, downsides, alternatives

Above we talk up the value of easy-to-run, easy-to-write, comprehensive automated tests.
This doesn't work for everything.  Sled Agent by nature takes over whatever system it's managing, and the resources it manages are not themselves virtualized, so it's not possible to fully test it outside of, say, giving it a whole VM.  (That said, thanks to Rust, it _is_ possible to have confidence in many mundane refactoring-type changes just by virtue of them compiling.)  Networking is another area where it's hard to automate testing in the way we've described above without a much richer virtualized environment.

A large, consolidated repo like Omicron has its downsides:

* It can be hard to find the stuff you need.
* If you try to build the whole thing, it can take a while.  There are tools for building parts of it.  People don't always know about these.  And they have their own pitfalls, like the way workspace feature unification works.  (See "Working with Omicron" in the README.)
* All things being equal, a larger repo repo means more developers in the same repo, meaning more merge conflicts.
* All things being equal, a full CI run takes longer, since it tests more.  (This can likely be mitigated with a merge queue, but we haven't tried that yet.)
* Putting a bunch of components into one repo can create the illusion that if you update both sides of a client/server interface, you're all set.  That's not true.  We need to consider when a newer client is deployed against an older server and vice versa.

The obvious alternative would be to use separate repos.  It's not clear this would address most of the above problems, though:

* With separate repos, it can be hard to find which _repo_ you need.
* The problem with workspace feature unification is that when you change what package you're building, you might find Cargo unexpectedly rebuilding some common dependency.  If we used separate repos, Cargo would _always_ rebuild that dependency when you switched repos.
* Merge conflicts for truly unrelated changes should be automatically resolved.
* Given that we need to create tooling to ensure that we manage both sides of stable APIs, we don't _also_ need to make it harder to _make_ such API changes by requiring coordinated pushes to two separate repos.

Some of us have worked on large systems that used separate repos for deployed components, particularly Joyent's https://github.com/TritonDataCenter/manta[Manta].  In that specific case, we had a bunch of challenges that led to the design goals above:

* There were https://github.com/TritonDataCenter/manta#repositories[10] - https://github.com/TritonDataCenter/manta/blob/master/docs/operator-guide/architecture.md#manta-components-at-a-glance[20] major top-level components, and https://github.com/TritonDataCenter/manta/blob/master/tools/jr-manifest.json[over 100 git repos] in all.  We eventually built a meta-repo with documentation about the whole system.  But since the active developers never needed this, it was often out of date.
* Each repo had its own process for setting up a development environment.  They would assume various tools on your PATH, but often not say where to get them or what version was required.  Even though almost all components used Node, they were bound to specific versions (due to fairly frequent breaking changes in Node), so you had to have the right version of Node on your path _for this repo_.
* Updating a common dependency (including Node) across the board involved separately updating it in each repo.  This was harder in Node than in Rust because breaking API changes only fail at runtime.  Plus, test coverage wasn't great (for all the reasons mentioned here), so this cost was even higher.
* Each repo had its own infrastructure for checking style and lint, running tests, etc.  Even though we had https://github.com/TritonDataCenter/eng/blob/master/docs/index.md[standardized on things like how to run these checks], and even provided a https://github.com/TritonDataCenter/eng/blob/master/docs/index.md#writing-makefiles[library of Makefiles] to make it easy to stick to this interface, in practice, every repo assumed different things about its environment.  These assumptions were not always documented.  When things failed, they often did so in baffling ways.
* Repo test suites generally assumed the developer had already started a copy of the service under test and any of its dependencies and had set some environment variables to point at it.

---

There are other annoying things about working on Omicron:

* Dependabot is incredibly noisy and annoying to work with for various reasons.  These are mostly implementation issues, and we could probably improve it considerably if we want to invest time in setting it up better (e.g., having it merge into a separate branch and merge _that_ branch into main once a week or so).
