// This Source Code Form is subject to the terms of the Mozilla Public
// License, v. 2.0. If a copy of the MPL was not distributed with this
// file, You can obtain one at https://mozilla.org/MPL/2.0/.

//! Keeper-specific types for the ClickHouse Admin Keeper API.

use super::config::{ClickhouseHost, RaftServerSettings};
use camino::Utf8PathBuf;
use daft::Diffable;
use derive_more::{Add, AddAssign, Display, From};
use omicron_common::api::external::Generation;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use std::collections::BTreeSet;
use std::net::Ipv6Addr;

use super::config::path_schema;

/// A unique ID for a ClickHouse Keeper
#[derive(
    Debug,
    Clone,
    Copy,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    From,
    Add,
    AddAssign,
    Display,
    JsonSchema,
    Serialize,
    Deserialize,
    Diffable,
)]
pub struct KeeperId(pub u64);

/// The top most type for configuring clickhouse-servers via
/// clickhouse-admin-keeper-api
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
pub struct KeeperConfigurableSettings {
    /// A unique identifier for the configuration generation.
    pub generation: Generation,
    /// Configurable settings for a ClickHouse keeper node.
    pub settings: KeeperSettings,
}

/// Configurable settings for a ClickHouse keeper node.
#[derive(Debug, Clone, PartialEq, Eq, Deserialize, Serialize, JsonSchema)]
#[serde(rename_all = "snake_case")]
pub struct KeeperSettings {
    /// Directory for the generated keeper configuration XML file
    #[schemars(schema_with = "path_schema")]
    pub config_dir: Utf8PathBuf,
    /// Unique ID of the keeper node
    pub id: KeeperId,
    /// ID and host of each server in the keeper cluster
    pub raft_servers: Vec<RaftServerSettings>,
    /// Directory for all files generated by ClickHouse itself
    #[schemars(schema_with = "path_schema")]
    pub datastore_path: Utf8PathBuf,
    /// Address the keeper is listening on
    pub listen_addr: Ipv6Addr,
}

#[derive(Debug, Clone, PartialEq, Eq, Deserialize, Serialize, JsonSchema)]
#[serde(rename_all = "snake_case")]
/// Logically grouped information file from a keeper node
pub struct Lgif {
    /// Index of the first log entry in the current log segment
    pub first_log_idx: u64,
    /// Term of the leader when the first log entry was created
    pub first_log_term: u64,
    /// Index of the last log entry in the current log segment
    pub last_log_idx: u64,
    /// Term of the leader when the last log entry was created
    pub last_log_term: u64,
    /// Index of the last committed log entry
    pub last_committed_log_idx: u64,
    /// Index of the last committed log entry from the leader's perspective
    pub leader_committed_log_idx: u64,
    /// Target index for log commitment during replication or recovery
    pub target_committed_log_idx: u64,
    /// Index of the most recent snapshot taken
    pub last_snapshot_idx: u64,
}

#[derive(
    Debug,
    Clone,
    PartialEq,
    Eq,
    Deserialize,
    Ord,
    PartialOrd,
    Serialize,
    JsonSchema,
)]
#[serde(rename_all = "snake_case")]
pub enum KeeperServerType {
    Participant,
    Learner,
}

#[derive(
    Debug,
    Clone,
    PartialEq,
    Eq,
    Deserialize,
    PartialOrd,
    Ord,
    Serialize,
    JsonSchema,
)]
#[serde(rename_all = "snake_case")]
pub struct KeeperServerInfo {
    /// Unique, immutable ID of the keeper server
    pub server_id: KeeperId,
    /// Host of the keeper server
    pub host: ClickhouseHost,
    /// Keeper server raft port
    pub raft_port: u16,
    /// A keeper server either participant or learner
    /// (learner does not participate in leader elections).
    pub server_type: KeeperServerType,
    /// non-negative integer telling which nodes should be
    /// prioritised on leader elections.
    /// Priority of 0 means server will never be a leader.
    pub priority: u16,
}

#[derive(
    Debug,
    Clone,
    PartialEq,
    Eq,
    Deserialize,
    PartialOrd,
    Ord,
    Serialize,
    JsonSchema,
)]
#[serde(rename_all = "snake_case")]
/// Keeper raft configuration information
pub struct RaftConfig {
    pub keeper_servers: BTreeSet<KeeperServerInfo>,
}

// While we generally use "Config", in this case we use "Conf"
// as it is the four letter word command we are invoking:
// `clickhouse keeper-client --q conf`
/// Keeper configuration information
#[derive(Debug, Clone, PartialEq, Eq, Deserialize, Serialize, JsonSchema)]
#[serde(rename_all = "snake_case")]
pub struct KeeperConf {
    /// Unique server id, each participant of the ClickHouse Keeper cluster must
    /// have a unique number (1, 2, 3, and so on).
    pub server_id: KeeperId,
    /// Whether Ipv6 is enabled.
    pub enable_ipv6: bool,
    /// Port for a client to connect.
    pub tcp_port: u16,
    /// Allow list of 4lw commands.
    pub four_letter_word_allow_list: String,
    /// Max size of batch in requests count before it will be sent to RAFT.
    pub max_requests_batch_size: u64,
    /// Min timeout for client session (ms).
    pub min_session_timeout_ms: u64,
    /// Max timeout for client session (ms).
    pub session_timeout_ms: u64,
    /// Timeout for a single client operation (ms).
    pub operation_timeout_ms: u64,
    /// How often ClickHouse Keeper checks for dead sessions and removes them (ms).
    pub dead_session_check_period_ms: u64,
    /// How often a ClickHouse Keeper leader will send heartbeats to followers (ms).
    pub heart_beat_interval_ms: u64,
    /// If the follower does not receive a heartbeat from the leader in this interval,
    /// then it can initiate leader election. Must be less than or equal to
    /// election_timeout_upper_bound_ms. Ideally they shouldn't be equal.
    pub election_timeout_lower_bound_ms: u64,
    /// If the follower does not receive a heartbeat from the leader in this interval,
    /// then it must initiate leader election.
    pub election_timeout_upper_bound_ms: u64,
    /// How many coordination log records to store before compaction.
    pub reserved_log_items: u64,
    /// How often ClickHouse Keeper will create new snapshots
    /// (in the number of records in logs).
    pub snapshot_distance: u64,
    /// Allow to forward write requests from followers to the leader.
    pub auto_forwarding: bool,
    /// Wait to finish internal connections and shutdown (ms).
    pub shutdown_timeout: u64,
    /// If the server doesn't connect to other quorum participants in the specified
    /// timeout it will terminate (ms).
    pub startup_timeout: u64,
    /// Text logging level about coordination (trace, debug, and so on).
    pub raft_logs_level: super::config::LogLevel,
    /// How many snapshots to keep.
    pub snapshots_to_keep: u64,
    /// How many log records to store in a single file.
    pub rotate_log_storage_interval: u64,
    /// Threshold when leader considers follower as stale and sends the snapshot
    /// to it instead of logs.
    pub stale_log_gap: u64,
    /// When the node became fresh.
    pub fresh_log_gap: u64,
    /// Max size in bytes of batch of requests that can be sent to RAFT.
    pub max_requests_batch_bytes_size: u64,
    /// Maximum number of requests that can be in queue for processing.
    pub max_request_queue_size: u64,
    /// Max size of batch of requests to try to get before proceeding with RAFT.
    /// Keeper will not wait for requests but take only requests that are already
    /// in the queue.
    pub max_requests_quick_batch_size: u64,
    /// Whether to execute read requests as writes through whole RAFT consesus with
    /// similar speed.
    pub quorum_reads: bool,
    /// Whether to call fsync on each change in RAFT changelog.
    pub force_sync: bool,
    /// Whether to write compressed coordination logs in ZSTD format.
    pub compress_logs: bool,
    /// Whether to write compressed snapshots in ZSTD format (instead of custom LZ4).
    pub compress_snapshots_with_zstd_format: bool,
    /// How many times we will try to apply configuration change (add/remove server)
    /// to the cluster.
    pub configuration_change_tries_count: u64,
    /// If connection to a peer is silent longer than this limit * (heartbeat interval),
    /// we re-establish the connection.
    pub raft_limits_reconnect_limit: u64,
    /// Path to coordination logs, just like ZooKeeper it is best to store logs
    /// on non-busy nodes.
    #[schemars(schema_with = "path_schema")]
    pub log_storage_path: Utf8PathBuf,
    /// Name of disk used for logs.
    pub log_storage_disk: String,
    /// Path to coordination snapshots.
    #[schemars(schema_with = "path_schema")]
    pub snapshot_storage_path: Utf8PathBuf,
    /// Name of disk used for storage.
    pub snapshot_storage_disk: String,
}

/// The configuration of the clickhouse keeper raft cluster returned from a
/// single keeper node
///
/// Each keeper is asked for its known raft configuration via `clickhouse-admin`
/// dropshot servers running in `ClickhouseKeeper` zones. state. We include the
/// leader committed log index known to the current keeper node (whether or not
/// it is the leader) to determine which configuration is newest.
#[derive(
    Clone,
    Debug,
    PartialEq,
    Eq,
    PartialOrd,
    Ord,
    Deserialize,
    Serialize,
    JsonSchema,
)]
#[serde(rename_all = "snake_case")]
pub struct ClickhouseKeeperClusterMembership {
    /// Keeper ID of the keeper being queried
    pub queried_keeper: KeeperId,
    /// Index of the last committed log entry from the leader's perspective
    pub leader_committed_log_index: u64,
    /// Keeper IDs of all keepers in the cluster
    pub raft_config: BTreeSet<KeeperId>,
}
